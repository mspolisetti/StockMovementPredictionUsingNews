{
  "cells": [
    {
      "metadata": {
        "_uuid": "61683b3d1cba7506e7d7aa867f1d4e815818e720"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "cc28a1e7d0a98f13bc76d6172516f65e07263890"
      },
      "cell_type": "markdown",
      "source": "# Template for Machine Learning Course Project"
    },
    {
      "metadata": {
        "_uuid": "28f22f01fe854ef209669a33db45bd58190e2036"
      },
      "cell_type": "markdown",
      "source": "\n***1. Feature Engineering***\n- Merge Market & News Data\n- Impute returns data using NOCB \n   https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n- Bin numerical to binary when there is not much data for factors.\n- Create new Features to account for Time Series auto Correlation between rows.\n\n***2. Data reduction & Exploration***\n- Subset of Data for top companies that always appear in news. We considered 15 companies with news data based on our research.\n\nThe reason for doing so was due to the abundant news articles as well as data available for those in particular. The five stocks we used were:\n      \n       'Barclays PLC'\\\n,'Citigroup Inc'\\\n,'Apple Inc'\\\n,'JPMorgan Chase & Co'\\\n,'Bank of America Corp'\\\n,'HSBC Holdings PLC'\\\n,'Goldman Sachs Group Inc'\\\n,'Deutsche Bank AG'\\\n,'BHP Billiton PLC'\\\n,'BP PLC'\\\n,'Google Inc'\\\n,'Boeing Co'\\\n,'Rio Tinto PLC'\\\n,'Royal Dutch Shell PLC'\\\n,'Ford Motor Co'\\\n,'General Electric Co'\\\n,'Morgan Stanley'\\\n,'Microsoft Corp'\\\n,'Exxon Mobil Corp'\\\n,'UBS AG'\\\n       \n- Subset of Data from 2013\n- Use numeric news data (Novelty, Volume counts, Sentececount, Relevance, takeSequence etc.) & returns data columns\n-  Spot outliers and plot correlation\n\n\n**3. Split train and Test**\n1. Transform target variable to binary Stock-Movement Up/Down (0/1)\n2. Stock-Movement Up/Down will be the label for training.\n3. TimeSeries Training is different from regular dataset training\nhttp://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html\n\n**4. Use Gradient Descent to tune parameters.**\n\n**5. Fit Classifier with Train**\nUsing Classifiers that work well with Mixed Data.\n\n    1. Random Forest\n    2. BaggingClassifier with DecisionTrees\n    3. XGBoost\n    4. Nueral Networks\n\n**6. Cross validation to estimate test error for this model.**\n\n**7. Use GridSearchCV to tune hyper parameters.**\n\n**8. Use the best estimator for test prediction and accuracy.**\n\n**Tips/Tricks: **\n\nMeasures to improve Test Accuracy of the models:\n1. Use companies that have data for VolumeCounts7D/NoveltyCount7D.\n\n2. Find outliers of all variables and treat them.\nhttps://www.kaggle.com/artgor/eda-feature-engineering-and-everything\n\n3 Create new features to capture autocorrelation:\n    e.g: https://www.kaggle.com/youhanlee/simple-quant-features-using-python\n    Make these specific to a particular Stock.\n    \n4. Use randomSearchCV instead of GridSearchCv\n\n5. HyperParameter Tuning\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\n6. Find means to add more data.\n\n7. Top 10 Features impacting next 10 day movement\n\n8. Splitting date into discrete components can allow decision trees were able to make better guesses.\n\n9. Make assetCode-specific datasets, train different assetCode specific models separtately on these datasets.\nCreate an ensemble of assetCode specific models?\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\nhttps://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n\n10. Overfit vs. Underfit curve plot\nhttp://scikit-learn.org/stable/modules/learning_curve.html\n"
    },
    {
      "metadata": {
        "_uuid": "4daf190e5465c842a9a33f77cafcd66a88171425"
      },
      "cell_type": "markdown",
      "source": "# Imports"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_kg_hide-input": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import *\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_inputMain, news_inputMain) = env.get_training_data()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91d3530d718108424a688de4882db8bc165e2b42",
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(market_inputMain.head())\nprint(news_inputMain.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "495b98c3a02b74dc266b1029f16800824e223216"
      },
      "cell_type": "code",
      "source": "for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_inputMain.loc[news_inputMain['sentimentClass'] == i, 'assetCodes']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b53769c1539e5e60cf926e71c3574a1d9a7529c5"
      },
      "cell_type": "code",
      "source": "df_volumeCount = news_inputMain.loc[news_inputMain['volumeCounts7D'] > 0, 'assetName']\nprint(f'Top mentioned companies for {j} volumeCounts7D are:')\nprint(df_volumeCount.value_counts().head(30))\nprint('')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "616740544ea149f7f51af0bf71c430d3c0ffff38",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4081c08e992e20171bb44c8218df739fb514397"
      },
      "cell_type": "markdown",
      "source": "# 1. Feature Engineering"
    },
    {
      "metadata": {
        "_uuid": "8f20e428c48c0c20fad30685d63453920d38a2cd"
      },
      "cell_type": "markdown",
      "source": "#### **Function to merge Market & News Datasets**"
    },
    {
      "metadata": {
        "_uuid": "dff38dc6d73fbea62c9b9a92d65e956e5d6a2c3f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Make a deep copy to keep the main dataset. Environment cannot be restarted at will.\ndfm = market_inputMain.copy(deep=True)\ndfn = news_inputMain.copy(deep=True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "20b5b2c2c1c0844a71ce104d2885a4c4d637af9e"
      },
      "cell_type": "code",
      "source": "#print(dfm[dfm[\"assetName\"].str.startswith('Goo')][\"assetName\"].unique())\n \ndfm[dfm[\"assetName\"].isin([ 'Apple Inc'\\\n     ,'JPMorgan Chase & Co'\\\n     ,'Bank of America Corp'\\\n     ,'HSBC Holdings PLC'\\\n     ,'Goldman Sachs Group Inc'\\\n     ,'Deutsche Bank AG'\\\n     ,'BHP Billiton PLC'\\\n     ,'BP PLC'\\\n     ,'Google Inc'\\\n     ,'Boeing Co'\\\n     ,'Rio Tinto PLC'\\\n     ,'Royal Dutch Shell PLC'\\\n     ,'Ford Motor Co'\\\n     ,'General Electric Co'\\\n     ,'Morgan Stanley'\\\n     ,'Microsoft Corp'\\\n     ,'Exxon Mobil Corp'\\\n     ,'UBS AG'\\\n     ,'Exxon Mobil Corp'\\\n     ,'UBS AG'\\\n    ,'Toyota Motor Corp'\\\n,'Royal Bank of Scotland Group PLC'\\\n,'Wal-Mart Stores Inc'\\\n,'BHP Billiton Ltd'\\\n,'General Motors Co'\\\n,'Verizon Communications Inc'\\\n,'AT&T Inc'\\\n,'Wells Fargo & Co'\\\n,'Amazon.com Inc'\\\n,'Lloyds Banking Group PLC'\n       ])]['assetCode'].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93d54c9d9d8f9efe5ab1c896481cd793ea1ce2b9"
      },
      "cell_type": "code",
      "source": " dfm = dfm[dfm[\"assetName\"].isin([   'Apple Inc'\\\n     ,'JPMorgan Chase & Co'\\\n     ,'Bank of America Corp'\\\n     ,'HSBC Holdings PLC'\\\n     ,'Goldman Sachs Group Inc'\\\n     ,'Deutsche Bank AG'\\\n     ,'BHP Billiton PLC'\\\n     ,'BP PLC'\\\n     ,'Google Inc'\\\n     ,'Boeing Co'\\\n     ,'Rio Tinto PLC'\\\n     ,'Royal Dutch Shell PLC'\\\n     ,'Ford Motor Co'\\\n     ,'General Electric Co'\\\n     ,'Morgan Stanley'\\\n     ,'Microsoft Corp'\\\n     ,'Exxon Mobil Corp'\\\n     ,'UBS AG'\\\n       ])]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "52261834b045a290f94b20fec74380430921d67a"
      },
      "cell_type": "markdown",
      "source": " ,'Intel Corp'\\\n     ,'Cisco Systems Inc'\\\n     ,'Oracle Corp'\\\n     ,'Microsoft Corp'\\\n     ,'International Business Machines Corp'\n     ,'Barclays PLC'\\\n     ,'Citigroup Inc'\\\n,'Barclays PLC'\\\n,'Citigroup Inc'\\\n,'Apple Inc'\\\n,'JPMorgan Chase & Co'\\\n,'Bank of America Corp'\\\n,'HSBC Holdings PLC'\\\n,'Goldman Sachs Group Inc'\\\n,'Deutsche Bank AG'\\\n,'BHP Billiton PLC'\\\n,'BP PLC'\\\n,'Google Inc'\\\n,'Boeing Co'\\\n,'Rio Tinto PLC'\\\n,'Royal Dutch Shell PLC'\\\n,'Ford Motor Co'\\\n,'General Electric Co'\\\n,'Morgan Stanley'\\\n,'Microsoft Corp'\\\n,'Exxon Mobil Corp'\\\n,'UBS AG'\\"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be86b4da74924c4b4011b288c1160ad03cc75434"
      },
      "cell_type": "code",
      "source": "market_inputMain.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7aa2bb88383e3e78133c3ff19bdcdaaa50b66d92",
        "trusted": true
      },
      "cell_type": "code",
      "source": "dfm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89bcf16010e6130650f251eae6532e1a9c1e254e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "market_inputMain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59820ebe22328fddf1b6bbed237a8eff9387a6c7"
      },
      "cell_type": "code",
      "source": "news_inputMain.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "453559e2da628c62312d277a0b49179edaaf2c4e",
        "trusted": true
      },
      "cell_type": "code",
      "source": "dfn.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a804644df8121dd91a463b67a8c0e6a280755d68"
      },
      "cell_type": "markdown",
      "source": "#### Cutdown datasets"
    },
    {
      "metadata": {
        "_uuid": "3ac49e29f5c09134760207979df55140e743bbc1",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#utc \nimport datetime\nimport pytz\n\nutc=pytz.UTC\n\n#cut down datasets to return\nstartdate = pd.to_datetime(\"2012-01-01\").replace(tzinfo=utc)\ndfm = dfm[dfm.time > startdate]\ndfn = dfn[dfn.time > startdate]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb9ce879de0ac1fb9433a949f23f62fc7417fa7e"
      },
      "cell_type": "markdown",
      "source": "#### EXPAND NEWS Dataset as each \"assetCodes\" field is a  list of assetCodes"
    },
    {
      "metadata": {
        "_uuid": "c3218d75cb4144a1efa280dd0569db83565c45ac",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#News dataset shape before expanding\nnews_df = dfn\nnews_df.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "445c7013ddaed86824f010ddefea291ec4466197",
        "trusted": true
      },
      "cell_type": "code",
      "source": "dfm.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8077c966f019c61f5377c5beb45c3cad42fc574b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#First five asset codes of non-expaned News Dataset\nnews_df[\"assetCodes\"].head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e5444e6efcd1084bb1c149e94e8c5d587e1a1fef",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Expanding assetCodes\nfrom itertools import chain\nnews_cols = news_df.columns.values\nnews_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")  \n#print(chain(*news_df['assetCodes']))\nassetCodes_expanded = list(chain(*news_df['assetCodes']))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0e5fba709f5b6a8677f8773a66f46a44d65035fe"
      },
      "cell_type": "code",
      "source": "dfm[\"assetCode\"].unique()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc00f68ca0f1f1ec3eab34300ec2808540250a7b"
      },
      "cell_type": "code",
      "source": "#assetCodes_expanded = assetCodes_expanded\n\nassetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\nassert len(assetCodes_index) == len(assetCodes_expanded)\nassetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\nassetCodes = assetCodes[assetCodes[\"assetCode\"].isin(['AAPL.O', 'AMZN.O', 'BA.N', 'BAC.N', 'BHP.N', 'BP.N', 'F.N',\n       'GE.N', 'GS.N', 'HBC.N', 'JPM.N', 'MS.N', 'MSFT.O', 'RDSa.N',\n       'RDSb.N', 'RTP.N', 'T.N', 'TM.N', 'VZ.N', 'WFC.N', 'XOM.N', 'DB.N',\n       'LYG.N', 'BBL.N', 'RBS.N', 'RIO.N', 'GM.N', 'HSBC.N'\n       ])]\nnews_df_expanded = pd.merge(assetCodes, news_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f4686abfef744e6d5e2e9d7789c073e23519570"
      },
      "cell_type": "markdown",
      "source": "'AAPL.O', 'BA.N', 'BAC.N', 'BBL.N', 'BCS.N', 'BP.N', 'DB.N', 'F.N',\\\n       'GE.N', 'GS.N', 'HBC.N', 'JPM.N', 'MS.N', 'MSFT.O', 'RDSa.N',\n       'RDSb.N', 'RTP.N', 'XOM.N', 'RIO.N', 'C.N', 'HSBC.N'"
    },
    {
      "metadata": {
        "_uuid": "04328c633436531c2c5b59a2c5c77f2232837ef9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Shape of news_df after expanding\nprint(news_df_expanded.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9067e8f7f1599df98e27407ec8f23e9031220046",
        "trusted": true
      },
      "cell_type": "code",
      "source": "news_df_expanded.iloc[:5, :10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "_uuid": "75dc4413973e648691d69f54a85bd8a9d1ed9f85",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Checking to see if there are missing values in news\nnews_df_expanded.isna().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d16c96c8b881bcd7a42e21976598c90322327a20"
      },
      "cell_type": "markdown",
      "source": "#### We found out that there are no missing values(NAs) in news dataset"
    },
    {
      "metadata": {
        "_uuid": "2fa06a4885e30e0222bb881e33ab7ed30dedb503"
      },
      "cell_type": "markdown",
      "source": "### Merge Market vs. News datasets"
    },
    {
      "metadata": {
        "_uuid": "5b1a29fba1b10ca5860b4d111e52dbe5d4f9eb35",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\"data_prep\" will do Merge and some basic cleaning\n\ndef data_prep(market_df,news_df):\n    asset_code_dict = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    columns_tobe_retained = ['time','assetCode', 'assetName' ,'volume', 'open', 'close','returnsClosePrevRaw1',\\\n                             'returnsOpenPrevRaw1','returnsClosePrevMktres1','returnsOpenPrevMktres1',\\\n                             'returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10',\\\n                             'returnsOpenPrevMktres10','returnsOpenNextMktres10',\\\n                             'assetCodeT','urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n           'firstCreated',   \\\n                      # 'asset_sentiment_count', 'asset_sentence_mean','len_audiences',\\\n           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\n    market_df['date'] = market_df['time'].dt.date\n    market_df['close_to_open'] = market_df['close'] / market_df['open']\n    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n    #News data feature creation\n    #news_df['time'] = news_df.time.dt.hour\n    #news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    #news_df['firstCreated'] = news_df['firstCreated'].dt.date \n    #news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    #news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    #news_df['len_audiences'] = news_df['audiences'].map(lambda x: len(eval(x)))\n    #kcol = ['firstCreated', 'assetCode']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    # Merge news and market data. Only keep numeric columns\n    market_df_merge = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n                            right_on=['firstCreated', 'assetCode'])\n\n    #return only data for the numeric columns + key information (assetCode, time)\n    return market_df_merge[columns_tobe_retained]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "876854d1b5f8ad0e26e606fbb99156848588b77f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Group News Data by firstCreated & assetCode\nkcol = ['firstCreated', 'assetCode']\nd = news_df_expanded.sort_values('firstCreated').copy(deep=True)\nd['firstCreated'] = d['firstCreated'].dt.date\nd = d.groupby(kcol, as_index=False).mean()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c02372dfc697f316352aefcba5114ec04fa2328",
        "trusted": true
      },
      "cell_type": "code",
      "source": "d.tail(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6eb14516991c8ff1c54b98ca9827aa23e2c3e178",
        "trusted": true
      },
      "cell_type": "code",
      "source": "dfmI = dfm.copy(deep=True)\ndfnI = d.copy(deep=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8b3473f184a66b4e3fad02bc35a4054bbd3b8beb"
      },
      "cell_type": "markdown",
      "source": "### Merge Market & News data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81445deb49d6b11f923b8129a9332cd04a6fc966"
      },
      "cell_type": "code",
      "source": "len(dfnI)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c9fc8c48d0447e28c09f671388bfe24f9184779",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#\"data_prep\" will do Merge of Market & News Datasets\nmerged_dataset = data_prep(dfmI,dfnI)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "068da2e003be7728cc1e07092eab3a0dad5d9272",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Look at missing value summary\nmerged_dataset.count()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b3dbeb1b017e3853c00e547d69e1831814fda13",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Checkingfor NAs\nmerged_dataset.isna().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "57278ae30674d737b0c7272a77830a9e262b768c",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Function to plot time series data\ndef plot_vs_time(data_frame, column, calculation='mean', span=10):\n    if calculation == 'mean':\n        group_temp = data_frame.groupby('firstCreated')[column].mean().reset_index()\n    if calculation == 'count':\n        group_temp = data_frame.groupby('firstCreated')[column].count().reset_index()\n    if calculation == 'nunique':\n        group_temp = data_frame.groupby('firstCreated')[column].nunique().reset_index()\n    group_temp = group_temp.ewm(span=span).mean()\n    fig = plt.figure(figsize=(10,3))\n    plt.plot(group_temp['firstCreated'], group_temp[column])\n    plt.xlabel('Time')\n    plt.ylabel(column)\n    plt.title('%s versus time' %column)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "32629545e8632589d91fe1786adf83712cab4cfb"
      },
      "cell_type": "markdown",
      "source": "### Look at NA values of returnsOpenPrevMktres10. We will re-verify this graph after interpolating to miss NA values in returns variables."
    },
    {
      "metadata": {
        "_uuid": "4dcac19d781ffd6f9499667021b5d8967417f912",
        "trusted": true
      },
      "cell_type": "code",
      "source": "d = merged_dataset[merged_dataset['assetCode'] == 'AAPL.O']\nimport matplotlib.pyplot as plt\n#print(d[d['returnsOpenPrevMktres10'].isna()]['assetCode'])\nprint(d.head())\nplt.plot(d['time'], d['returnsOpenPrevMktres10'])\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9b34bea6789024eca62338f8acfea6ff58cb7e3c"
      },
      "cell_type": "markdown",
      "source": "#### Impute Market Returns data using  (NOCB) imputation: https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4"
    },
    {
      "metadata": {
        "_uuid": "0e18a0ca38b033db7b25b057eb8374d9895324b4"
      },
      "cell_type": "markdown",
      "source": "### Fill nan values in MktRes columns using NOCB interpolation."
    },
    {
      "metadata": {
        "_uuid": "b18f280c956ef077ceee6d208eea009e2f691cde",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fill nan values in MktRes columns using NOCB interpolation.\nmarket_fill = merged_dataset.copy(deep=True)\ncolumn_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_market)):\n    market_fill[column_market[i]].interpolate(method='nearest', inplace=True)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c5be40978249ff72ce43257055ac92657614271",
        "trusted": true
      },
      "cell_type": "code",
      "source": "market_fill[market_fill['assetCode'] == 'AAPL.O'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "08ad3d7b80f41cb0a8cb2f57ef65ffa74b6cacfa"
      },
      "cell_type": "markdown",
      "source": "### Checking to make sure NAs are filled"
    },
    {
      "metadata": {
        "_uuid": "3008c223e557cdfbaa0fba8f315b2fd0953f3129",
        "trusted": true
      },
      "cell_type": "code",
      "source": "d = market_fill[market_fill['assetCode'] == 'AAPL.O']\nimport matplotlib.pyplot as plt\n#print(d[d['returnsOpenPrevMktres10'].isna()]['assetCode'])\nprint(d.head())\nplt.plot(d['time'], d['returnsOpenPrevMktres10'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9e4fd8c515e7cb35f389d06223844f5d9450d25a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Checkingfor NAs before extracting data only for the 5 companies\nmarket_fill.isna().sum()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b524e8b4d8c22d4e35dbe7c07e649fa2e75ac73e"
      },
      "cell_type": "markdown",
      "source": "# 2. Data Reduction to include only 10 Companies"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d764a9c1c4a6769f73a1167e5095aacc7af919d9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a6d04cf4b3eb82674e92d81512a2ef45ab4d1267"
      },
      "cell_type": "markdown",
      "source": "##### Almost all of them have NAs"
    },
    {
      "metadata": {
        "_uuid": "ae8adea4f434d6a846315c703b0e740fd268a6f7"
      },
      "cell_type": "markdown",
      "source": "# 1. Feature Engineering Contd."
    },
    {
      "metadata": {
        "_uuid": "81065902ea992c03f3140ef2d958f7a11e2ed2ca"
      },
      "cell_type": "markdown",
      "source": "#### Bin numerical to binary when there is not much data for factors.\n"
    },
    {
      "metadata": {
        "_uuid": "6b577d6bbed1e4c20c77d3878c9f4ce4ff87f8f3"
      },
      "cell_type": "markdown",
      "source": "#### Create new Features to account for Time Series auto Correlation between rows."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24981847ecef4b85129762a0d93611828507fa6a"
      },
      "cell_type": "code",
      "source": "def rsiFunc(prices, n=14):\n    deltas = np.diff(prices)\n    seed = deltas[:n+1]\n    up = seed[seed>=0].sum()/n\n    down = -seed[seed<0].sum()/n\n    rs = up/down\n    rsi = np.zeros_like(prices)\n    rsi[:n] = 100. - 100./(1.+rs)\n\n    for i in range(n, len(prices)):\n        delta = deltas[i-1] # cause the diff is 1 shorter\n\n        if delta>0:\n            upval = delta\n            downval = 0.\n        else:\n            upval = 0.\n            downval = -delta\n\n        up = (up*(n-1) + upval)/n\n        down = (down*(n-1) + downval)/n\n\n        rs = up/down\n        rsi[i] = 100. - 100./(1.+rs)\n\n    return rsi",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "70fe65fdffb63fa71593fd640e208613206ab8e2"
      },
      "cell_type": "code",
      "source": "#'AAPL.O',  'CSCO.O', 'IBM.N', 'INTC.O', 'MSFT.O', 'ORCL.O', 'ORCL.N'\nfull_dataset = pd.DataFrame()\nfor assetCode in ['AAPL.O', 'BA.N', 'BAC.N', 'BCS.N', 'BP.N', 'CSCO.O', 'F.N',\n       'GE.N', 'GS.N', 'HBC.N', 'IBM.N', 'INTC.O', 'JPM.N', 'MS.N',\n       'MSFT.O', 'ORCL.O', 'RDSa.N', 'RDSb.N', 'RTP.N', 'XOM.N', 'DB.N',\n       'BBL.N', 'RIO.N', 'C.N', 'HSBC.N', 'ORCL.N']:\n   df = pd.DataFrame()\n   # Gather asset specific data\n   df = market_fill[market_fill[\"assetCode\"] == assetCode]\n   df['rsi20D'] = rsiFunc(df['close'].values, 20)\n   #Calculating all the trend variables for this assetCode\n   df['volume10DMA'] = df[\"volume\"].rolling(window=10).mean() \n   #Create new feature for close price moving average.\n   df['close10DMA'] = df['close'].rolling(window=10).mean()\n   df['sentenceCount_20DMA'] = df['sentenceCount'].rolling(window=7).mean()\n   df['firstMentionSentence_20DMA'] = df['firstMentionSentence'].rolling(window=7).mean()\n   df['relevance_20DMA'] = df['relevance'].rolling(window=7).mean()\n   df['sentimentWordCount_20DMA'] = df['sentimentWordCount'].rolling(window=7).mean()\n   df['sentimentClass_20DMA'] = df['sentimentClass'].rolling(window=7).mean()\n   #Exponential Moving Average\n   ewma = pd.Series.ewm\n   df['close_10EMA'] = ewma(df[\"close\"], span=10).mean()\n   #Bollinger Bands are a type of statistical chart characterizing the prices and \n   #volatility over time of a financial instrument or commodity, using a formulaic method \n   #propounded by John Bollinger in the 1980s. Financial traders employ these charts as \n   #a methodical tool to inform trading decisions, control automated trading systems, \n   #or as a component of technical analysis. Bollinger Bands display a graphical band \n   #(the envelope maximum and minimum of moving averages, similar to\n   #Keltner or Donchian channels) and volatility (expressed by the width of the envelope) \n   #in one two-dimensional chart.\n\n   #ref. https://en.wikipedia.org/wiki/Bollinger_Bands \n   #Moving average convergence divergence (MACD) is a trend-following momentum indicator that shows the \n   #relationship between two moving averages of prices.\n   #The MACD is calculated by subtracting the 26-day exponential moving average (EMA) from the 12-day EMA\n   df['close_26EMA'] = ewma(df[\"close\"], span=26).mean()\n   df['close_12EMA'] = ewma(df[\"close\"], span=12).mean()\n   df['MACD'] = df['close_12EMA'] - df['close_26EMA']\n   no_of_std = 2\n   #ref. https://www.investopedia.com/terms/m/macd.asp\n\n   df['MA_10MA'] = df['close'].rolling(window=10).mean()\n   df['MA_10MA_std'] = df['close'].rolling(window=10).std() \n   df['MA_10MA_BB_high'] = df['MA_10MA'] + no_of_std * df['MA_10MA_std']\n   df['MA_10MA_BB_low'] = df['MA_10MA'] - no_of_std * df['MA_10MA_std']\n   full_dataset = full_dataset.append(df)\n\n\n\nfull_dataset[\"firstCreated\"] = full_dataset[\"time\"].dt.date\nfull_dataset['Year'] = full_dataset.time.dt.year\nfull_dataset['Month'] = full_dataset.time.dt.month\nfull_dataset['Day'] = full_dataset.time.dt.day\nfull_dataset['Week'] = full_dataset.time.dt.week\nfull_dataset = full_dataset.sort_values('firstCreated')\n#ref. https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/RSI\n#full_dataset['rsi10D'] = rsiFunc(full_dataset['close'].values, 10)\n\n#The Relative Strength Index (RSI) to identify general trend.\n\n#full_dataset[full_dataset['urgency'].isna()][\"assetName\"].unique()\n#Look for missing values in news and which Companies have NAs\nprint(full_dataset.isna().sum())\n\n#Create new feature for volume moving average.\n#full_dataset['volume10DMA'] = full_dataset['volume'].rolling(window=10).mean()\n\n#Create new feature for close price moving average.\n#full_dataset['close10DMA'] = full_dataset['close'].rolling(window=10).mean()\n\n#The Relative Strength Index (RSI) to identify general trend.\n#ref. https://www.fidelity.com/learning-center/trading-investing/technical-analysis/technical-indicator-guide/RSI\n#full_dataset['rsi10D'] = rsiFunc(full_dataset['close'].values, 10)\n\n#Exponential Moving Average\n#ewma = pd.Series.ewm\n#full_dataset['close_10EMA'] = ewma(full_dataset[\"close\"], span=10).mean()\n\n#Bollinger Bands are a type of statistical chart characterizing the prices and \n#volatility over time of a financial instrument or commodity, using a formulaic method \n#propounded by John Bollinger in the 1980s. Financial traders employ these charts as \n#a methodical tool to inform trading decisions, control automated trading systems, \n#or as a component of technical analysis. Bollinger Bands display a graphical band \n#(the envelope maximum and minimum of moving averages, similar to\n#Keltner or Donchian channels) and volatility (expressed by the width of the envelope) \n#in one two-dimensional chart.\n\n#ref. https://en.wikipedia.org/wiki/Bollinger_Bands\n\n\n\n#Moving average convergence divergence (MACD) is a trend-following momentum indicator that shows the \n#relationship between two moving averages of prices.\n#The MACD is calculated by subtracting the 26-day exponential moving average (EMA) from the 12-day EMA\n#full_dataset['close_26EMA'] = ewma(full_dataset[\"close\"], span=26).mean()\n#full_dataset['close_12EMA'] = ewma(full_dataset[\"close\"], span=12).mean()\n\n#full_dataset['MACD'] = full_dataset['close_12EMA'] - full_dataset['close_26EMA']\n#no_of_std = 2\n#ref. https://www.investopedia.com/terms/m/macd.asp\n\n#full_dataset['MA_10MA'] = full_dataset['close'].rolling(window=10).mean()\n#full_dataset['MA_10MA_std'] = full_dataset['close'].rolling(window=10).std() \n#full_dataset['MA_10MA_BB_high'] = full_dataset['MA_10MA'] + no_of_std * full_dataset['MA_10MA_std']\n#full_dataset['MA_10MA_BB_low'] = full_dataset['MA_10MA'] - no_of_std * full_dataset['MA_10MA_std']\n \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7a736253c34a663401da3333004bdad7aa7078e"
      },
      "cell_type": "code",
      "source": "#Lets fill NAs with NOCB\n# Fill nan values in News Variables\n \ncolumn_market = [\"urgency\", \"takeSequence\",\"companyCount\",\"marketCommentary\",\n\"sentenceCount\",\"firstMentionSentence\",\"relevance\",\n\"sentimentClass\",\"sentimentWordCount\",\"noveltyCount24H\",\n\"firstCreated\",\"noveltyCount3D\",\"noveltyCount5D\",\n\"noveltyCount7D\",\"volumeCounts24H\",\"volumeCounts3D\"\n,\"volumeCounts5D\",\"volumeCounts7D\"]\n#column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_market)):\n    full_dataset[column_market[i]].interpolate(method='linear', inplace=True) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "68dc3358f46987d501c956d74ff203632b7a27c1"
      },
      "cell_type": "markdown",
      "source": "#### Spot outlier Companies for close/open price difference"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2fd8d6e0db5edb35895dfcc7adaf1e37b0f843f7"
      },
      "cell_type": "markdown",
      "source": "market_train_df = full_dataset.copy(deep=True)\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\ngrouped.sort_values(('price_diff', 'std'), ascending=False)[:10].head()\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2518733432e866c0c2328d99e3a51d501aa637fc"
      },
      "cell_type": "markdown",
      "source": "g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')"
    },
    {
      "metadata": {
        "_uuid": "a2f65f2db531741bc335b58f1d42a0e472369ba3"
      },
      "cell_type": "markdown",
      "source": "### No surprising outliers to remove in terms of prices"
    },
    {
      "metadata": {
        "_uuid": "f53e81694de43d2f900420255072ba4f72823584"
      },
      "cell_type": "markdown",
      "source": "#### Plot Correlations of Market Data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0143f1356fe19af794b76c661eb262412b0f606a"
      },
      "cell_type": "code",
      "source": "market_train_df = full_dataset.copy(deep=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b35e5abc81a614bb13a9ea5a6f1d839f2aef7dec",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import seaborn as sns\nmarket_train_df[\"target_stockMoveUp\"] = market_train_df.returnsOpenNextMktres10 > 0\ncolumns_corr_market = ['volume', 'returnsClosePrevRaw1','returnsOpenPrevRaw1',\\\n           'returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevRaw10','MA_10MA_BB_high', 'MA_10MA_BB_low'\\\n          , 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'volume10DMA', 'close10DMA','rsi10D','close_10EMA','MACD',\\\n                       'target_stockMoveUp']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(market_train_df[columns_corr_market].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "253e69e46e05c97cf423456accf8996156bea468"
      },
      "cell_type": "markdown",
      "source": "**Conclusions:**\n\n1. Stock volumes have some positive impact on the Stock movement.\n2. All of the returns variable have positive correlation with each other.\n3. Close & Open prices have strong correlation."
    },
    {
      "metadata": {
        "_uuid": "793f6db4f0b0977b1b7823f851ee1983b1165bf2",
        "trusted": true
      },
      "cell_type": "code",
      "source": "columns_corr_merge = ['volume','open','close','returnsOpenPrevRaw1','returnsOpenPrevMktres1'\\\n                     ,'returnsOpenPrevRaw10' ,'returnsOpenPrevMktres10','target_stockMoveUp'\\\n                      ,'noveltyCount7D','volumeCounts7D' ]\ncolormap = plt.cm.RdBu\n# Scaling \ndf = market_train_df[columns_corr_merge]\nmins = np.min(df, axis=0)\nmaxs = np.max(df, axis=0)\nrng = maxs - mins\ndf = 1 - ((maxs - df) / rng)\n\nplt.figure(figsize=(18,15))\nsns.heatmap(df.astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation market and news')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3cd54cb092fc7d772201d4b03310826e7946140c",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "**Conclusions:**\n\n1. Stock volumes have positive correlation with Stock Movement variables and the news Novelty/Volume.\n2. Novelty of the content seems to have correlation with Stock Closing Price.\n3. Novelty Indicators and Volume counts are postively correlated with each other.\n"
    },
    {
      "metadata": {
        "_uuid": "3c65113d21d67b431863885de2802e698dcd422e"
      },
      "cell_type": "markdown",
      "source": "# 3. Split Train and Test"
    },
    {
      "metadata": {
        "_uuid": "497e081c23b10f75312cbde22fdebac3c7889ae9",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1 = full_dataset.copy(deep=True)\ny = df1.returnsOpenNextMktres10 > 0\n# Rest of the dataset is X\ncols = ['Year', 'Day', 'Week', 'Month', 'volume', 'close', 'open' \\\n        ,'returnsOpenPrevMktres10',  'rsi20D', 'MA_10MA_BB_high','MA_10MA_BB_low','MACD'\\\n        ,'volumeCounts7D','takeSequence','assetCodeT'\\\n        ,'sentenceCount_20DMA','firstMentionSentence_20DMA','relevance_20DMA'\\\n        ,'sentimentWordCount','sentimentClass_20DMA'\n       ]\nX = df1[cols] \ntrain_size = int(len(X) * 0.66)\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\ny_train, y_test = y[0:train_size], y[train_size:len(X)]\n\nprint('Observations: %d' % (len(X)))\nprint('Training Observations: %d' % (len(X_train)))\nprint('Testing Observations: %d' % (len(X_test)))\n\n# The target is binary\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "64f8145eaae6cd9ab1941dd63b70e2f7d7a6b328"
      },
      "cell_type": "markdown",
      "source": "# 5. Fit Classifier after Cross Validation using GridSearchCV"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "3763a9f579091b17261b9fe06eff9df9fc4d8e77"
      },
      "cell_type": "code",
      "source": "import xgboost as xgb\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport numpy as np\n \nxgb_model = xgb.XGBClassifier()\n ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a20406c802a12242c3ddd998b3c34bd1174e7b15"
      },
      "cell_type": "markdown",
      "source": "## Training Accuracy"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac8eb71cae934f56590e69b7b32969a32c1a480d"
      },
      "cell_type": "code",
      "source": "import matplotlib.pylab as plt\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nfrom sklearn.metrics  import accuracy_score\nxgb_model.fit(X_train,y_train)\nplot_importance(xgb_model, max_num_features=20) # top 10 most important features\nplt.show()\naccuracy_score(xgb_model.predict(X_train),y_train)\n\n#DTC.fit(X_train,y_train)\n#plot_importance(DTC, max_num_features=20) # top 10 most important features\n#plt.show()\n#accuracy_score(DTC.predict(X_train),y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1c2cd37ac83a1b06fe274aed601e0529d1d12adf"
      },
      "cell_type": "markdown",
      "source": "# **6. Cross validation - Rolling Cross Validation for TimeSeries data.**\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3318f15aec5cee4f26d6b1ecfdd8a0110653d00c"
      },
      "cell_type": "code",
      "source": "#brute force scan for all parameters, here are the tricks\n#usually max_depth is 6,7,8\n#learning rate is around 0.05, but small changes may make big diff\n#tuning min_child_weight subsample colsample_bytree can have \n#much fun of fighting against overfit \n#n_estimators is how many round of boosting\n#finally, ensemble xgboost with multiple seeds may reduce variance\nfrom sklearn.model_selection import TimeSeriesSplit\ntss = TimeSeriesSplit(n_splits=10).split(X_train)\n\nparams = {\n       'min_child_weight': [1, 5, 10],\n       'gamma': [1.5],\n       # 'subsample': [0.6, 0.8, 1.0],\n       # 'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,6],\n        'n_estimators': [100]\n        }\n\ngsearch = RandomizedSearchCV(xgb_model, params, n_jobs=5,cv=tss, scoring='accuracy',verbose=1, refit=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0284161b3a820506e5a47d2e67197ad66040ace5"
      },
      "cell_type": "markdown",
      "source": "# # **7. Use GridSearchCV to tune hyper parameters.**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a141dbf21013ad1b9b5df34a7ed469237852948",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "import warnings\nwarnings.filterwarnings('ignore')\ngsearch.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7d3eb27032210e4c5f8308bb38cc0d328c8cd083"
      },
      "cell_type": "markdown",
      "source": "# 8. Validation set accuracy."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "66d401057d5c8ddefadf7d75e0fb941cd9a56ae9"
      },
      "cell_type": "code",
      "source": "#from sklearn.metrics import accuracy_score\n#trust your CV!\nbest_parameters, score, _ = max(gsearch.grid_scores_, key=lambda x: x[1])\nprint('Cross validation Accuracy score:', score)\nfor param_name in sorted(best_parameters.keys()):\n    print(\"%s: %r\" % (param_name, best_parameters[param_name]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2beeb4c74c6dbb5d7160c50cd0e9661a76f34e5"
      },
      "cell_type": "markdown",
      "source": "# 9. Predict Test  and calculate accuracy"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0d328baf4b7af8591b4726283b656979a1c1f89"
      },
      "cell_type": "code",
      "source": "accuracy_score(gsearch.predict(X_test), y_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6763ac64e42f2bd49d50ad4d42c2400fbc53897b"
      },
      "cell_type": "markdown",
      "source": "# Build NeuralNet"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fde39ddae77c1b0e9ccb4018b2541fda9cb503d"
      },
      "cell_type": "markdown",
      "source": "cat_cols = ['sentimentClass','assetCodeT', 'relevance','takeSequence']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', \\\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\\\n                    'returnsOpenPrevMktres10','noveltyCount7D','volumeCounts7D']"
    },
    {
      "metadata": {
        "_uuid": "fb6671d0bda8474bda00b39d7ca715a2f7446a1c"
      },
      "cell_type": "markdown",
      "source": "# Handling Catogerical Variables"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "141269d5c6ba78d35ea348ee524579fe8f3a1604"
      },
      "cell_type": "markdown",
      "source": "def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(X_train.loc[:, cat].astype(str).unique())}\n    X_train[cat] = X_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ee9fcf05cd5eaacda2b1a0a318da3f44e3afa8a"
      },
      "cell_type": "markdown",
      "source": "from sklearn.preprocessing import StandardScaler\n \nX_train[num_cols] = X_train[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1dadf58384f5069facef74b9d7b695771e2f0df1"
      },
      "cell_type": "markdown",
      "source": "from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy, mse\n\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n#categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Flatten()(categorical_embeddings[0])\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\nnumerical_inputs = Input(shape=(11,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(128,activation='relu')(numerical_logits)\nnumerical_logits = Dense(64,activation='relu')(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(64,activation='relu')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)"
    },
    {
      "metadata": {
        "_uuid": "c2dd3ea4e3100c88a47891571b935f4286510e14"
      },
      "cell_type": "markdown",
      "source": "# Train NN Model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "252f58f94eccd8e2700af0da9f4a5c2645753266"
      },
      "cell_type": "markdown",
      "source": "from keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=5,verbose=True)\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_train,y_train.astype(int)),\n          epochs=2,\n          verbose=True,\n          callbacks=[early_stop,check_point]) "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9ff7032be091ac3e00c6b0723c4ad749d7661710"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7fdf7c55e88a2fcf4779feddffa9664bc1738ca1"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}