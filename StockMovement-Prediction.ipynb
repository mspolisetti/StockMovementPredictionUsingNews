{"cells":[{"metadata":{"_uuid":"61683b3d1cba7506e7d7aa867f1d4e815818e720"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"cc28a1e7d0a98f13bc76d6172516f65e07263890"},"cell_type":"markdown","source":"# Template for Machine Learning Course Project"},{"metadata":{"_uuid":"28f22f01fe854ef209669a33db45bd58190e2036"},"cell_type":"markdown","source":"\n***1. Feature Engineering***\n- Merge Market & News Data\n- Impute returns data using NOCB \n   https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4\n- Bin numerical to binary when there is not much data for factors.\n- Create new Features to account for Time Series auto Correlation between rows.\n\n***2. Data reduction & Exploration***\n- Subset of Data for top companies that always appear in news. We considered 15 companies with news data based on our research.\n\nThe reason for doing so was due to the abundant news articles as well as data available for those in particular. The five stocks we used were:\n      \n       'Barclays PLC'\\\n,'Citigroup Inc'\\\n,'Apple Inc'\\\n,'JPMorgan Chase & Co'\\\n,'Bank of America Corp'\\\n,'HSBC Holdings PLC'\\\n,'Goldman Sachs Group Inc'\\\n,'Deutsche Bank AG'\\\n,'BHP Billiton PLC'\\\n,'BP PLC'\\\n,'Google Inc'\\\n,'Boeing Co'\\\n,'Rio Tinto PLC'\\\n,'Royal Dutch Shell PLC'\\\n,'Ford Motor Co'\\\n,'General Electric Co'\\\n,'Morgan Stanley'\\\n,'Microsoft Corp'\\\n,'Exxon Mobil Corp'\\\n,'UBS AG'\\\n       \n- Subset of Data from 2013\n- Use numeric news data (Novelty, Volume counts, Sentececount, Relevance, takeSequence etc.) & returns data columns\n-  Spot outliers and plot correlation\n\n\n**3. Split train and Test**\n1. Transform target variable to binary Stock-Movement Up/Down (0/1)\n2. Stock-Movement Up/Down will be the label for training.\n3. TimeSeries Training is different from regular dataset training\nhttp://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html\n\n**4. Compare various Classifiers to find the best one.**\nhttps://www.kaggle.com/aldemuro/comparing-ml-algorithms-train-accuracy-90\n**5. Fit Classifier with Train**\nUsing Classifiers that work well with Mixed Data.\n\n    1. Random Forest\n    2. BaggingClassifier with DecisionTrees\n    3. XGBoost\n    4. Nueral Networks\n\n**6. Cross validation to estimate test error for this model.**\n\n**7. Use GridSearchCV to tune hyper parameters.**\n\n**8. Use the best estimator for test prediction and accuracy.**\n\n**Tips/Tricks: **\n\nMeasures to improve Test Accuracy of the models:\n1. Use companies that have data for VolumeCounts7D/NoveltyCount7D.\n\n2. Find outliers of all variables and treat them.\nhttps://www.kaggle.com/artgor/eda-feature-engineering-and-everything\n\n3 Create new features to capture autocorrelation:\n    e.g: https://www.kaggle.com/youhanlee/simple-quant-features-using-python\n    Make these specific to a particular Stock.\n    \n4. Use randomSearchCV instead of GridSearchCv\n\n5. HyperParameter Tuning\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\n6. Find means to add more data.\n\n7. Top 10 Features impacting next 10 day movement\n\n8. Splitting date into discrete components can allow decision trees were able to make better guesses.\n\n9. Make assetCode-specific datasets, train different assetCode specific models separtately on these datasets.\nCreate an ensemble of assetCode specific models?\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\nhttps://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/\n\n10. Overfit vs. Underfit curve plot\nhttp://scikit-learn.org/stable/modules/learning_curve.html\nhttps://www.kaggle.com/danbrice/keras-plot-history-full-report-and-grid-search\n\n11. Fit models with various subsets of the input features to find the best model.\nhttp://www.science.smith.edu/~jcrouser/SDS293/labs/lab8-py.html\n\n12. XGBoost overfit\nhttps://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/"},{"metadata":{"_uuid":"4daf190e5465c842a9a33f77cafcd66a88171425"},"cell_type":"markdown","source":"# Imports"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import *\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_inputMain, news_inputMain) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91d3530d718108424a688de4882db8bc165e2b42","trusted":true},"cell_type":"code","source":"#print(market_inputMain.head())\n#print(news_inputMain.head())\nmarket_train_df = market_inputMain.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8bc2a41a08b62450c34379407d3dcef372f8834"},"cell_type":"code","source":"data = []\nfor i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    price_df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n\n    data.append(go.Scatter(\n        x = price_df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n        y = price_df['close'].values,\n        name = f'{i} quantile'\n    ))\nlayout = go.Layout(dict(title = \"Trends of closing prices by quantiles\",\n                  xaxis = dict(title = 'Month'),\n                  yaxis = dict(title = 'Price (USD)'),\n                  ),legend=dict(\n                orientation=\"h\"),\n    annotations=[\n        dict(\n            x='2008-09-01 22:00:00+0000',\n            y=82,\n            xref='x',\n            yref='y',\n            text='Collapse of Lehman Brothers',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2011-08-01 22:00:00+0000',\n            y=85,\n            xref='x',\n            yref='y',\n            text='Black Monday',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2014-10-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Another crisis',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=-20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        ),\n        dict(\n            x='2016-01-01 22:00:00+0000',\n            y=120,\n            xref='x',\n            yref='y',\n            text='Oil prices crash',\n            showarrow=True,\n            font=dict(\n                family='Courier New, monospace',\n                size=16,\n                color='#ffffff'\n            ),\n            align='center',\n            arrowhead=2,\n            arrowsize=1,\n            arrowwidth=2,\n            arrowcolor='#636363',\n            ax=20,\n            ay=-30,\n            bordercolor='#c7c7c7',\n            borderwidth=2,\n            borderpad=4,\n            bgcolor='#ff7f0e',\n            opacity=0.8\n        )\n    ])\npy.iplot(dict(data=data, layout=layout), filename='basic-line')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"495b98c3a02b74dc266b1029f16800824e223216","_kg_hide-output":false},"cell_type":"code","source":"for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n    df_sentiment = news_inputMain.loc[news_inputMain['sentimentClass'] == i, 'assetName']\n    print(f'Top mentioned companies for {j} sentiment are:')\n    print(df_sentiment.value_counts().head(5))\n    print('')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b53769c1539e5e60cf926e71c3574a1d9a7529c5"},"cell_type":"code","source":"df_volumeCount = news_inputMain.loc[news_inputMain['volumeCounts7D'] > 0, 'assetName']\nprint(f'Top mentioned companies for {j} volumeCounts7D are:')\nprint(df_volumeCount.value_counts()[1:50])\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"616740544ea149f7f51af0bf71c430d3c0ffff38","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4081c08e992e20171bb44c8218df739fb514397"},"cell_type":"markdown","source":"# 1. Feature Engineering"},{"metadata":{"_uuid":"8f20e428c48c0c20fad30685d63453920d38a2cd"},"cell_type":"markdown","source":"#### **Function to merge Market & News Datasets**"},{"metadata":{"_uuid":"dff38dc6d73fbea62c9b9a92d65e956e5d6a2c3f","trusted":true},"cell_type":"code","source":"#Make a deep copy to keep the main dataset. Environment cannot be restarted at will.\ndfm = market_inputMain.copy(deep=True)\ndfn = news_inputMain.copy(deep=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc725b02f6de038b9eb175d9ee38a85da864b40d"},"cell_type":"code","source":"dfm[\"assetCode\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93d54c9d9d8f9efe5ab1c896481cd793ea1ce2b9"},"cell_type":"code","source":" dfm = dfm[dfm[\"assetName\"].isin([  \n'Citigroup Inc'\\\n,'Apple Inc'\\\n,'JPMorgan Chase & Co'\\\n,'Bank of America Corp'\\\n,'HSBC Holdings PLC'\\\n,'Goldman Sachs Group Inc'\\\n,'Deutsche Bank AG'\\\n,'BHP Billiton PLC'\\\n,'BP PLC'\\\n,'Google Inc'\\\n,'Boeing Co'\\\n,'Rio Tinto PLC'\\\n,'Royal Dutch Shell PLC'\\\n,'Ford Motor Co'\\\n,'General Electric Co'\\\n,'Morgan Stanley'\\\n,'Microsoft Corp'\\\n,'Exxon Mobil Corp'\\\n,'UBS AG'\\\n,'Toyota Motor Corp'\\\n,'Royal Bank of Scotland Group PLC'\\\n,'Wal-Mart Stores Inc'\\\n,'BHP Billiton Ltd'\\\n,'General Motors Co'\\\n,'Verizon Communications Inc'\\\n,'AT&T Inc'\\\n,'Wells Fargo & Co'\\\n,'Amazon.com Inc'\\\n,'Lloyds Banking Group PLC'\\\n,'Credit Suisse Group AG'\\\n,'Chevron Corp'\\\n,'Pfizer Inc'\\\n,'American International Group Inc'\\\n,'Vodafone Group PLC'\\\n,'Federal Home Loan Mortgage Corp'\\\n,'Sony Corp'\\\n,'Federal National Mortgage Association'\\\n,'Total SA'\\\n,'Motors Liquidation Co'\\\n,'Nokia Oyj'\\\n,'Intel Corp'\\\n,'Twenty-First Century Fox Inc'\\\n,'Yahoo! Inc'\\\n,'International Business Machines Corp'\\\n,'GlaxoSmithKline PLC'\\\n,'Credit Suisse AG'\\\n,'Facebook Inc'\\\n,'HP Inc'\\\n,'Banco Santander SA'  ])]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52261834b045a290f94b20fec74380430921d67a"},"cell_type":"markdown","source":",'Royal Bank of Scotland Group PLC'\\\n,'Wal-Mart Stores Inc'\\\n,'BHP Billiton Ltd'\\\n,'General Motors Co'\\\n,'Verizon Communications Inc'\\\n,'AT&T Inc'\\\n,'Wells Fargo & Co'\\\n,'Amazon.com Inc'\\\n,'Lloyds Banking Group PLC'\\\n,'BlackBerry Ltd'\\\n,'Eni SpA'\\\n,'Lockheed Martin Corp'\\\n,'ConocoPhillips'\\\n,'Honda Motor Co Ltd'\\\n,'Statoil ASA'\\\n,'AstraZeneca PLC'\\\n,'Novartis AG'\\\n,'Petroleo Brasileiro SA Petrobras'\\\n,'Blackstone Group LP'\\\n,'Walt Disney Co'\\\n,'Johnson & Johnson'\\\n,'Sanofi SA'\\\n,'Vale SA'\\\n,'Nasdaq Inc'\\\n,'Cisco Systems Inc'\\\n,'Anheuser Busch Inbev SA'\\\n,'Comcast Corp'\\\n,'Berkshire Hathaway Inc'\\\n,'Delta Air Lines Inc'\\\n,'CME Group Inc'\\\n,'Arconic Inc'\\\n,'United Technologies Corp'\\\n,'Merck & Co Inc'\\\n,'Time Warner Inc'\\\n,'Caterpillar Inc'\\\n,'Oracle Corp'\\\n,'Viacom Inc'\\\n,'Target Corp'\\\n,'Telefonica SA'\\\n,'Dell Inc'\\\n,'Bank of Montreal'\\\n,'Siemens AG'\\\n,'Barrick Gold Corp'\\\n,'Royal Bank of Canada'\\\n,'eBay Inc'\\\n,'Twitter Inc'\\\n,'Netflix Inc'\\\n,'Northrop Grumman Corp'\\\n,'Aviva PLC'\\\n,'NYSE Euronext'\\\n,'Telefonaktiebolaget LM Ericsson'\\\n,'Coca-Cola Co'\\\n,'American Airlines Group Inc'\\\n,'Procter & Gamble Co'\\\n,'Eli Lilly and Co'\\\n,'United Continental Holdings Inc'\\\n,'Anadarko Petroleum Corp'\\\n,'PetroChina Co Ltd'\\\n,'Bank of New York Mellon Corp'\\\n,'BlackRock Inc'\\\n,'Latam Airlines Group SA'\\\n,'Chesapeake Energy Corp'\\\n,'Sprint Communications Inc'\\\n,'China Petroleum & Chemical Corp'\\\n,'Telecom Italia SpA'\\\n,'Halliburton Co'\\\n,'Bristol-Myers Squibb Co'\\\n,'Valeant Pharmaceuticals International Inc'\\\n,'Valero Energy Corp'\\\n,'Dow Chemical Co'\\\n,'J C Penney Company Inc'\\\n,'Enbridge Inc'\\\n,'ArcelorMittal SA'\\\n,'Qualcomm Inc'\\\n,'American Express Co'\\\n,'Prudential PLC'\\\n,'Potash Corporation of Saskatchewan Inc'\\\n,\"McDonald's Corp\"\\\n,'Merrill Lynch & Co Inc'\\\n,'S&P Global Inc'\\\n,'Suncor Energy Inc'\\\n,\"Macy's Inc\"\\\n,'Toronto-Dominion Bank'\\\n,'ING Groep NV'\\"},{"metadata":{"trusted":true,"_uuid":"be86b4da74924c4b4011b288c1160ad03cc75434"},"cell_type":"code","source":"market_inputMain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7aa2bb88383e3e78133c3ff19bdcdaaa50b66d92","trusted":true},"cell_type":"code","source":"dfm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89bcf16010e6130650f251eae6532e1a9c1e254e","trusted":true},"cell_type":"code","source":"market_inputMain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59820ebe22328fddf1b6bbed237a8eff9387a6c7"},"cell_type":"code","source":"news_inputMain.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"453559e2da628c62312d277a0b49179edaaf2c4e","trusted":true},"cell_type":"code","source":"dfn.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a804644df8121dd91a463b67a8c0e6a280755d68"},"cell_type":"markdown","source":"#### Cutdown datasets"},{"metadata":{"_uuid":"3ac49e29f5c09134760207979df55140e743bbc1","trusted":true},"cell_type":"code","source":"#utc \nimport datetime\nimport pytz\n\nutc=pytz.UTC\n\n#cut down datasets to return\nstartdate = pd.to_datetime(\"2014-01-01\").replace(tzinfo=utc)\ndfm = dfm[dfm.time > startdate]\ndfn = dfn[dfn.time > startdate]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb9ce879de0ac1fb9433a949f23f62fc7417fa7e"},"cell_type":"markdown","source":"#### EXPAND NEWS Dataset as each \"assetCodes\" field is a  list of assetCodes"},{"metadata":{"_uuid":"c3218d75cb4144a1efa280dd0569db83565c45ac","trusted":true},"cell_type":"code","source":"#News dataset shape before expanding\nnews_df = dfn\nnews_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"445c7013ddaed86824f010ddefea291ec4466197","trusted":true},"cell_type":"code","source":"dfm.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8077c966f019c61f5377c5beb45c3cad42fc574b","trusted":true},"cell_type":"code","source":"#First five asset codes of non-expaned News Dataset\nnews_df[\"assetCodes\"].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5444e6efcd1084bb1c149e94e8c5d587e1a1fef","trusted":true},"cell_type":"code","source":"#Expanding assetCodes\nfrom itertools import chain\nnews_cols = news_df.columns.values\nnews_df['assetCodes'] = news_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")  \n#print(chain(*news_df['assetCodes']))\nassetCodes_expanded = list(chain(*news_df['assetCodes']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e5fba709f5b6a8677f8773a66f46a44d65035fe"},"cell_type":"code","source":"assetCodeArray = dfm[\"assetCode\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc00f68ca0f1f1ec3eab34300ec2808540250a7b"},"cell_type":"code","source":"#assetCodes_expanded = assetCodes_expanded\n\nassetCodes_index = news_df.index.repeat( news_df['assetCodes'].apply(len) )\nassert len(assetCodes_index) == len(assetCodes_expanded)\nassetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\nassetCodes = assetCodes[assetCodes[\"assetCode\"].isin(assetCodeArray)]\nnews_df_expanded = pd.merge(assetCodes, news_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f4686abfef744e6d5e2e9d7789c073e23519570"},"cell_type":"markdown","source":"'AAPL.O', 'BA.N', 'BAC.N', 'BBL.N', 'BCS.N', 'BP.N', 'DB.N', 'F.N',\\\n       'GE.N', 'GS.N', 'HBC.N', 'JPM.N', 'MS.N', 'MSFT.O', 'RDSa.N',\n       'RDSb.N', 'RTP.N', 'XOM.N', 'RIO.N', 'C.N', 'HSBC.N'"},{"metadata":{"_uuid":"04328c633436531c2c5b59a2c5c77f2232837ef9","trusted":true},"cell_type":"code","source":"#Shape of news_df after expanding\nprint(news_df_expanded.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9067e8f7f1599df98e27407ec8f23e9031220046","trusted":true},"cell_type":"code","source":"news_df_expanded.iloc[:5, :10]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"75dc4413973e648691d69f54a85bd8a9d1ed9f85","trusted":true},"cell_type":"code","source":"#Checking to see if there are missing values in news\nnews_df_expanded.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d16c96c8b881bcd7a42e21976598c90322327a20"},"cell_type":"markdown","source":"#### We found out that there are no missing values(NAs) in news dataset"},{"metadata":{"_uuid":"2fa06a4885e30e0222bb881e33ab7ed30dedb503"},"cell_type":"markdown","source":"### Merge Market vs. News datasets"},{"metadata":{"_uuid":"5b1a29fba1b10ca5860b4d111e52dbe5d4f9eb35","trusted":true},"cell_type":"code","source":"#\"data_prep\" will do Merge and some basic cleaning\n\ndef data_prep(market_df,news_df):\n    asset_code_dict = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n    columns_tobe_retained = ['time','assetCode', 'assetName' ,'volume', 'open', 'close','returnsClosePrevRaw1',\\\n                             'returnsOpenPrevRaw1','returnsClosePrevMktres1','returnsOpenPrevMktres1',\\\n                             'returnsClosePrevRaw10','returnsOpenPrevRaw10','returnsClosePrevMktres10',\\\n                             'returnsOpenPrevMktres10','returnsOpenNextMktres10',\\\n                             'assetCodeT','urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n           'firstCreated',   \\\n                      # 'asset_sentiment_count', 'asset_sentence_mean','len_audiences',\\\n           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\n    market_df['date'] = market_df['time'].dt.date\n    market_df['close_to_open'] = market_df['close'] / market_df['open']\n    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n    #News data feature creation\n    #news_df['time'] = news_df.time.dt.hour\n    #news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n    #news_df['firstCreated'] = news_df['firstCreated'].dt.date \n    #news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n    #news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n    #news_df['len_audiences'] = news_df['audiences'].map(lambda x: len(eval(x)))\n    #kcol = ['firstCreated', 'assetCode']\n    news_df = news_df.groupby(kcol, as_index=False).mean()\n\n    # Merge news and market data. Only keep numeric columns\n    market_df_merge = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n                            right_on=['firstCreated', 'assetCode'])\n\n    #return only data for the numeric columns + key information (assetCode, time)\n    return market_df_merge[columns_tobe_retained]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"876854d1b5f8ad0e26e606fbb99156848588b77f","trusted":true},"cell_type":"code","source":"#Group News Data by firstCreated & assetCode\nkcol = ['firstCreated', 'assetCode']\nd = news_df_expanded.sort_values('firstCreated').copy(deep=True)\nd['firstCreated'] = d['firstCreated'].dt.date\nd = d.groupby(kcol, as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c02372dfc697f316352aefcba5114ec04fa2328","trusted":true},"cell_type":"code","source":"d.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eb14516991c8ff1c54b98ca9827aa23e2c3e178","trusted":true},"cell_type":"code","source":"dfmI = dfm.copy(deep=True)\ndfnI = d.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b3473f184a66b4e3fad02bc35a4054bbd3b8beb"},"cell_type":"markdown","source":"### Merge Market & News data"},{"metadata":{"trusted":true,"_uuid":"81445deb49d6b11f923b8129a9332cd04a6fc966"},"cell_type":"code","source":"len(dfnI)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c9fc8c48d0447e28c09f671388bfe24f9184779","trusted":true},"cell_type":"code","source":"#\"data_prep\" will do Merge of Market & News Datasets\nmerged_dataset = data_prep(dfmI,dfnI)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"068da2e003be7728cc1e07092eab3a0dad5d9272","trusted":true},"cell_type":"code","source":"#Look at missing value summary\nmerged_dataset.count()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b3dbeb1b017e3853c00e547d69e1831814fda13","trusted":true},"cell_type":"code","source":"#Checkingfor NAs\nmerged_dataset.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57278ae30674d737b0c7272a77830a9e262b768c","trusted":true},"cell_type":"code","source":"# Function to plot time series data\ndef plot_vs_time(data_frame, column, calculation='mean', span=10):\n    if calculation == 'mean':\n        group_temp = data_frame.groupby('firstCreated')[column].mean().reset_index()\n    if calculation == 'count':\n        group_temp = data_frame.groupby('firstCreated')[column].count().reset_index()\n    if calculation == 'nunique':\n        group_temp = data_frame.groupby('firstCreated')[column].nunique().reset_index()\n    group_temp = group_temp.ewm(span=span).mean()\n    fig = plt.figure(figsize=(10,3))\n    plt.plot(group_temp['firstCreated'], group_temp[column])\n    plt.xlabel('Time')\n    plt.ylabel(column)\n    plt.title('%s versus time' %column)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32629545e8632589d91fe1786adf83712cab4cfb"},"cell_type":"markdown","source":"### Look at NA values of returnsOpenPrevMktres10. We will re-verify this graph after interpolating to miss NA values in returns variables."},{"metadata":{"trusted":true,"_uuid":"c751a0a2cd3db20edd21618e3b7ef0cd175fe00c"},"cell_type":"code","source":"merged_dataset[merged_dataset['returnsOpenPrevMktres10'].isna()]['assetCode']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dcac19d781ffd6f9499667021b5d8967417f912","trusted":true},"cell_type":"code","source":"d = merged_dataset[merged_dataset['assetCode'] == 'SAN.N']\nimport matplotlib.pyplot as plt\n#print(d[d['returnsOpenPrevMktres10'].isna()]['assetCode'])\nprint(d.head())\nplt.plot(d['time'], d['returnsOpenPrevMktres10'])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b34bea6789024eca62338f8acfea6ff58cb7e3c"},"cell_type":"markdown","source":"#### Impute Market Returns data using  (NOCB) imputation: https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4"},{"metadata":{"_uuid":"0e18a0ca38b033db7b25b057eb8374d9895324b4"},"cell_type":"markdown","source":"### Fill nan values in MktRes columns using NOCB interpolation."},{"metadata":{"_uuid":"b18f280c956ef077ceee6d208eea009e2f691cde","trusted":true},"cell_type":"code","source":"# Fill nan values in MktRes columns using NOCB interpolation.\nmarket_fill = merged_dataset.copy(deep=True)\ncolumn_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\ncolumn_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_market)):\n    market_fill[column_market[i]].interpolate(method='nearest', inplace=True)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08ad3d7b80f41cb0a8cb2f57ef65ffa74b6cacfa"},"cell_type":"markdown","source":"### Checking to make sure NAs are filled"},{"metadata":{"_uuid":"3008c223e557cdfbaa0fba8f315b2fd0953f3129","trusted":true},"cell_type":"code","source":"d = market_fill[market_fill['assetCode'] == 'SAN.N']\nimport matplotlib.pyplot as plt\n#print(d[d['returnsOpenPrevMktres10'].isna()]['assetCode'])\nprint(d.head())\nplt.plot(d['time'], d['returnsOpenPrevMktres10'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e4fd8c515e7cb35f389d06223844f5d9450d25a","trusted":true},"cell_type":"code","source":"#Checkingfor NAs before extracting data only for the 5 companies\nmarket_fill.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6c51b337955c9b22c30b6a7342f8b3c0bfcb0e7","scrolled":false},"cell_type":"code","source":"#fill nulls with NAs\nimport numpy as np\nmarket_fill = market_fill.fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9470d309e130ea148279bbbbcbfa4cbf60f0e4ee"},"cell_type":"code","source":"market_fill.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d649a76ecd61cecc324c41ff502d1e910390b26"},"cell_type":"code","source":"market_fill.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d764a9c1c4a6769f73a1167e5095aacc7af919d9"},"cell_type":"code","source":"#Lets fill NAs with Linear Interpolation\n# Fill nan values in News Variables\n \ncolumn_market = [\"urgency\", \"takeSequence\",\"companyCount\",\"marketCommentary\",\n\"sentenceCount\",\"firstMentionSentence\",\"relevance\",\n\"sentimentClass\",\"sentimentWordCount\",\"noveltyCount24H\",\n\"firstCreated\",\"noveltyCount3D\",\"noveltyCount5D\",\n\"noveltyCount7D\",\"volumeCounts24H\",\"volumeCounts3D\"\n,\"volumeCounts5D\",\"volumeCounts7D\"]\n#column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\nfor i in range(len(column_market)):\n    market_fill[column_market[i]].interpolate(method='nearest', inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6d04cf4b3eb82674e92d81512a2ef45ab4d1267"},"cell_type":"markdown","source":"##### Almost all of them have NAs"},{"metadata":{"_uuid":"ae8adea4f434d6a846315c703b0e740fd268a6f7"},"cell_type":"markdown","source":"# 1. Feature Engineering Contd."},{"metadata":{"_uuid":"81065902ea992c03f3140ef2d958f7a11e2ed2ca"},"cell_type":"markdown","source":"#### Bin numerical to binary when there is not much data for factors.\n"},{"metadata":{"_uuid":"6b577d6bbed1e4c20c77d3878c9f4ce4ff87f8f3"},"cell_type":"markdown","source":"#### Create new Features to account for Time Series auto Correlation between rows."},{"metadata":{"trusted":true,"_uuid":"24981847ecef4b85129762a0d93611828507fa6a"},"cell_type":"code","source":"def rsiFunc(prices, n=14):\n    deltas = np.diff(prices)\n    seed = deltas[:n+1]\n    up = seed[seed>=0].sum()/n\n    down = -seed[seed<0].sum()/n\n    rs = up/down\n    rsi = np.zeros_like(prices)\n    rsi[:n] = 100. - 100./(1.+rs)\n\n    for i in range(n, len(prices)):\n        delta = deltas[i-1] # cause the diff is 1 shorter\n\n        if delta>0:\n            upval = delta\n            downval = 0.\n        else:\n            upval = 0.\n            downval = -delta\n\n        up = (up*(n-1) + upval)/n\n        down = (down*(n-1) + downval)/n\n\n        rs = up/down\n        rsi[i] = 100. - 100./(1.+rs)\n\n    return rsi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"70fe65fdffb63fa71593fd640e208613206ab8e2","scrolled":false},"cell_type":"code","source":"#'AAPL.O',  'CSCO.O', 'IBM.N', 'INTC.O', 'MSFT.O', 'ORCL.O', 'ORCL.N'\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime\nfull_dataset = pd.DataFrame()\n\n\nfor assetCode in assetCodeArray:\n   df = pd.DataFrame()\n   # Gather asset specific data\n   df = market_fill[market_fill[\"assetCode\"] == assetCode]\n   df['rsi10D'] = rsiFunc(df['close'].values, 10)\n   #Calculating all the trend variables for this assetCode\n   df['volume10DMA'] = df[\"volume\"].rolling(window=10,min_periods=1).mean() \n   df['returnsClosePrevMktres1-10DMA'] = df[\"returnsClosePrevMktres1\"].rolling(window=10,min_periods=1).mean()\n   df['returnsClosePrevMktres1-yearly'] = df[\"returnsClosePrevMktres1\"].rolling(window=365,min_periods=1).mean()\n   df['returnsClosePrevMktres1-quarterly'] = df[\"returnsClosePrevMktres1\"].rolling(window=90,min_periods=1).mean()\n\n   df['returnsOpenPrevMktres1-10DMA'] = df[\"returnsOpenPrevMktres1\"].rolling(window=10,min_periods=1).mean()\n   df['returnsOpenPrevMktres1-yearly'] = df[\"returnsOpenPrevMktres1\"].rolling(window=365,min_periods=1).mean()\n   df['returnsOpenPrevMktres1-quarterly'] = df[\"returnsOpenPrevMktres1\"].rolling(window=90,min_periods=1).mean()\n   \n   #Create new feature for close price moving average.\n   df['close10DMA'] = df['close'].rolling(window=10,min_periods=1).mean()\n   df['sentenceCount7D'] = df['sentenceCount'].rolling(window=7,min_periods=1).sum()\n   df['firstMentionSentence7D'] = df['firstMentionSentence'].rolling(window=7,min_periods=1).sum()\n   df['relevance7D'] = df['relevance'].rolling(window=7,min_periods=1).sum()\n   df['sentimentWordCount7D'] = df['sentimentWordCount'].rolling(window=7,min_periods=1).sum()\n   df['sentimentClass7D'] = df['sentimentClass'].rolling(window=7,min_periods=1).sum()\n   df['urgency7D'] = df['urgency'].rolling(window=7,min_periods=1).sum()\n   df['takeSequence7D'] = df['takeSequence'].rolling(window=7,min_periods=1).sum()\n   df['companyCount7D'] = df['companyCount'].rolling(window=7,min_periods=1).sum()\n   df['marketCommentary7D'] = df['marketCommentary'].rolling(window=7,min_periods=1).sum()\n   #df['bodySize7D'] = df['bodySize'].rolling(window=7).sum()\n\n   #Exponential Moving Average\n   ewma = pd.Series.ewm\n   df['close_10EMA'] = ewma(df[\"close\"], span=10).mean()\n   #Bollinger Bands are a type of statistical chart characterizing the prices and \n   #volatility over time of a financial instrument or commodity, using a formulaic method \n   #propounded by John Bollinger in the 1980s. Financial traders employ these charts as \n   #a methodical tool to inform trading decisions, control automated trading systems, \n   #or as a component of technical analysis. Bollinger Bands display a graphical band \n   #(the envelope maximum and minimum of moving averages, similar to\n   #Keltner or Donchian channels) and volatility (expressed by the width of the envelope) \n   #in one two-dimensional chart.\n\n   #ref. https://en.wikipedia.org/wiki/Bollinger_Bands \n   #Moving average convergence divergence (MACD) is a trend-following momentum indicator that shows the \n   #relationship between two moving averages of prices.\n   #The MACD is calculated by subtracting the 26-day exponential moving average (EMA) from the 12-day EMA\n   df['close_26EMA'] = ewma(df[\"close\"], span=26).mean()\n   df['close_12EMA'] = ewma(df[\"close\"], span=12).mean()\n   df['MACD'] = df['close_12EMA'] - df['close_26EMA']\n   no_of_std = 2\n   #ref. https://www.investopedia.com/terms/m/macd.asp\n\n   df['MA_10MA'] = df['close'].rolling(window=10,min_periods=1).mean()\n   df['MA_10MA_std'] = df['close'].rolling(window=10,min_periods=1).std() \n   df['MA_10MA_BB_high'] = df['MA_10MA'] + no_of_std * df['MA_10MA_std']\n   df['MA_10MA_BB_low'] = df['MA_10MA'] - no_of_std * df['MA_10MA_std']\n   full_dataset = full_dataset.append(df)\n\n\n\nfull_dataset[\"firstCreated\"] = full_dataset[\"time\"].dt.date\nfull_dataset['Year'] = full_dataset.time.dt.year\nfull_dataset['Month'] = full_dataset.time.dt.month\nfull_dataset['Day'] = full_dataset.time.dt.day\nfull_dataset['Week'] = full_dataset.time.dt.week\n\nimport datetime\nfull_dataset['day_of_year']  = full_dataset[\"time\"].dt.dayofyear\nfull_dataset = full_dataset.sort_values('firstCreated')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68dc3358f46987d501c956d74ff203632b7a27c1"},"cell_type":"markdown","source":"#### Spot outlier Companies for close/open price difference"},{"metadata":{"trusted":true,"_uuid":"d601854c2eae7a24c8de981f6d51a040b4a9cf4c"},"cell_type":"code","source":"full_dataset[\"firstCreated\"] = full_dataset[\"time\"].dt.date\nfull_dataset['Year'] = full_dataset.time.dt.year\nfull_dataset['Month'] = full_dataset.time.dt.month\nfull_dataset['Day'] = full_dataset.time.dt.day\nfull_dataset['Week'] = full_dataset.time.dt.week\nimport datetime\nfull_dataset['day_of_year']  = full_dataset[\"time\"].dt.dayofyear\nfull_dataset['quarter']  = full_dataset[\"time\"].dt.quarter\nfull_dataset = full_dataset.sort_values('firstCreated')\n\nfull_dataset[\"daily_diff\"] = full_dataset[\"close\"] - full_dataset[\"open\"]\nfull_dataset['close_to_open'] =  np.abs(full_dataset['close'] / full_dataset['open'])\n    \n # determine whether the day is set on a holiday\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as cal\nholidays = cal().holidays(start='2007-01-01', end='2018-09-27').to_pydatetime()\nfull_dataset['on_holiday'] = full_dataset[\"firstCreated\"].str.slice(0,10).apply(lambda x: 1 if x in holidays else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51652dc3e5ed95ab2bb8200d04681a8740e37b28"},"cell_type":"markdown","source":" plot_vs_time(full_dataset, 'returnsClosePrevMktres1', calculation='mean', span=30)\n plot_vs_time(full_dataset, 'returnsClosePrevMktres1', calculation='mean', span=90)"},{"metadata":{"trusted":true,"_uuid":"c76a491dd96ed35746936b754e9bea6d376619f9"},"cell_type":"markdown","source":"full_dataset[full_dataset[\"assetCode\"] == 'AAL.O'][['firstCreated','returnsClosePrevMktres1', 'returnsClosePrevMktres1-20DMA', 'returnsClosePrevMktres1-quarterly']].head(30)\n"},{"metadata":{"trusted":true,"_uuid":"e566fc6f236a0cd97ab1660066492583fd70993e"},"cell_type":"code","source":"full_dataset.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb32718e5416bb8cdf61f82da0352c3c21d624c2"},"cell_type":"code","source":"pos = len(full_dataset[full_dataset['returnsOpenNextMktres10']>0])\nneg = len(full_dataset[full_dataset['returnsOpenNextMktres10']<0])\ntot = pos + neg\n\nprint ('Positive cases %: ', pos  * 100/tot)\nprint ('Negative cases %: ', neg  * 100/tot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fd8d6e0db5edb35895dfcc7adaf1e37b0f843f7"},"cell_type":"code","source":"market_train_df = full_dataset.copy(deep=True)\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\ngrouped.sort_values(('price_diff', 'std'), ascending=False)[:10].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2518733432e866c0c2328d99e3a51d501aa637fc"},"cell_type":"code","source":"g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\ng['min_text'] = 'Maximum price drop: ' + (-1 * g['price_diff']['min']).astype(str)\ntrace = go.Scatter(\n    x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n    y = g['price_diff']['std'].values,\n    mode='markers',\n    marker=dict(\n        size = g['price_diff']['std'].values,\n        color = g['price_diff']['std'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = g['min_text'].values\n    #text = f\"Maximum price drop: {g['price_diff']['min'].values}\"\n    #g['time'].dt.strftime(date_format='%Y-%m-%d').values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Top 10 months by standard deviation of price change within a day',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'price_diff',\n        ticklen= 5,\n        gridwidth= 2,\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2f65f2db531741bc335b58f1d42a0e472369ba3"},"cell_type":"markdown","source":"### No surprising outliers to remove in terms of prices"},{"metadata":{"_uuid":"f53e81694de43d2f900420255072ba4f72823584"},"cell_type":"markdown","source":"#### Plot Correlations of Market Data"},{"metadata":{"_uuid":"b35e5abc81a614bb13a9ea5a6f1d839f2aef7dec","trusted":true},"cell_type":"markdown","source":"import seaborn as sns\nmarket_train_df[\"target_stockMoveUp\"] = market_train_df.returnsOpenNextMktres10 > 0\ncolumns_corr_market =  [ 'volume', 'open', 'close',\\\n       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\\\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\\\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\\\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\\\n       'close_10EMA', 'close_26EMA', 'close_12EMA', 'MACD',\\\n       'MA_10MA', 'MA_10MA_std', 'MA_10MA_BB_high', 'MA_10MA_BB_low','target_stockMoveUp']\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(18,15))\nsns.heatmap(market_train_df[columns_corr_market].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation')\nplt.rcParams['font.size'] = 10"},{"metadata":{"_uuid":"253e69e46e05c97cf423456accf8996156bea468"},"cell_type":"markdown","source":"*Conclusions:**\n\n1. Stock volumes have some positive impact on the Stock movement.\n2. All of the returns variable have positive correlation with each other.\n3. Close & Open prices have strong correlation."},{"metadata":{"_uuid":"793f6db4f0b0977b1b7823f851ee1983b1165bf2","trusted":true},"cell_type":"markdown","source":"columns_corr_merge = [    \n       'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts24H',\n       'volumeCounts3D', 'volumeCounts5D', 'volumeCounts7D', \n       'volume10DMA', 'close10DMA', 'sentenceCount7D',\n       'firstMentionSentence7D', 'relevance7D', 'sentimentWordCount7D',\n       'sentimentClass7D' , 'urgency7D', 'takeSequence7D', 'companyCount7D',\n       'marketCommentary7D','target_stockMoveUp']\ncolormap = plt.cm.RdBu\n# Scaling \ndf = market_train_df[columns_corr_merge]\nmins = np.min(df, axis=0)\nmaxs = np.max(df, axis=0)\nrng = maxs - mins\ndf = 1 - ((maxs - df) / rng)\n\nplt.figure(figsize=(18,15))\nsns.heatmap(df.astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\nplt.title('Pair-wise correlation market and news')\nplt.rcParams['font.size'] = 10"},{"metadata":{"_uuid":"3cd54cb092fc7d772201d4b03310826e7946140c","trusted":false},"cell_type":"markdown","source":"**Conclusions:**\n\n1. Stock volumes have positive correlation with Stock Movement variables and the news Novelty/Volume.\n2. Novelty of the content seems to have correlation with Stock Closing Price.\n3. Novelty Indicators and Volume counts are postively correlated with each other.\n"},{"metadata":{"_uuid":"3c65113d21d67b431863885de2802e698dcd422e"},"cell_type":"markdown","source":"# Functions"},{"metadata":{"trusted":true,"_uuid":"b02fffe9ca001e3b9df8c0b13a81d1dbb247fdde"},"cell_type":"code","source":"def plot_classification_report(cr, title='Classification report ', with_avg_total=False, cmap=plt.cm.Blues):\n\n    lines = cr.split('\\n')\n\n    classes = []\n    plotMat = []\n    for line in lines[2 : (len(lines) - 3)]:\n        #print(line)\n        t = line.split()\n        # print(t)\n        classes.append(t[0])\n        v = [float(x) for x in t[1: len(t) - 1]]\n        print(v)\n        plotMat.append(v)\n\n    if with_avg_total:\n        aveTotal = lines[len(lines) - 1].split()\n        classes.append('avg/total')\n        vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n        plotMat.append(vAveTotal)\n\n\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    x_tick_marks = np.arange(3)\n    y_tick_marks = np.arange(len(classes))\n    plt.xticks(x_tick_marks, ['precision', 'recall', 'f1-score'], rotation=45)\n    plt.yticks(y_tick_marks, classes)\n    plt.tight_layout()\n    plt.ylabel('Classes')\n    plt.xlabel('Measures')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"606657e7bee223a2fbb27957d202102a7c1a5a7b"},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nplot a pretty confusion matrix with seaborn\nCreated on Mon Jun 25 14:17:37 2018\n@author: Wagner Cipriano - wagnerbhbr - gmail - CEFETMG / MMC\nREFerences:\n  https://www.mathworks.com/help/nnet/ref/plotconfusion.html\n  https://stackoverflow.com/questions/28200786/how-to-plot-scikit-learn-classification-report\n  https://stackoverflow.com/questions/5821125/how-to-plot-confusion-matrix-with-string-axis-rather-than-integer-in-python\n  https://www.programcreek.com/python/example/96197/seaborn.heatmap\n  https://stackoverflow.com/questions/19233771/sklearn-plot-confusion-matrix-with-labels/31720054\n  http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n\"\"\"\n\n#imports\nfrom pandas import DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom matplotlib.collections import QuadMesh\nimport seaborn as sn\n\n\ndef get_new_fig(fn, figsize=[10,10]):\n    \"\"\" Init graphics \"\"\"\n    fig1 = plt.figure(fn, figsize)\n    ax1 = fig1.gca()   #Get Current Axis\n    ax1.cla() # clear existing plot\n    return fig1, ax1\n#\n\ndef configcell_text_and_colors(array_df, lin, col, oText, facecolors, posi, fz, fmt, show_null_values=0):\n    \"\"\"\n      config cell text and colors\n      and return text elements to add and to dell\n      @TODO: use fmt\n    \"\"\"\n    text_add = []; text_del = [];\n    cell_val = array_df[lin][col]\n    tot_all = array_df[-1][-1]\n    per = (float(cell_val) / tot_all) * 100\n    curr_column = array_df[:,col]\n    ccl = len(curr_column)\n\n    #last line  and/or last column\n    if(col == (ccl - 1)) or (lin == (ccl - 1)):\n        #tots and percents\n        if(cell_val != 0):\n            if(col == ccl - 1) and (lin == ccl - 1):\n                tot_rig = 0\n                for i in range(array_df.shape[0] - 1):\n                    tot_rig += array_df[i][i]\n                per_ok = (float(tot_rig) / cell_val) * 100\n            elif(col == ccl - 1):\n                tot_rig = array_df[lin][lin]\n                per_ok = (float(tot_rig) / cell_val) * 100\n            elif(lin == ccl - 1):\n                tot_rig = array_df[col][col]\n                per_ok = (float(tot_rig) / cell_val) * 100\n            per_err = 100 - per_ok\n        else:\n            per_ok = per_err = 0\n\n        per_ok_s = ['%.2f%%'%(per_ok), '100%'] [per_ok == 100]\n\n        #text to DEL\n        text_del.append(oText)\n\n        #text to ADD\n        font_prop = fm.FontProperties(weight='bold', size=fz)\n        text_kwargs = dict(color='w', ha=\"center\", va=\"center\", gid='sum', fontproperties=font_prop)\n        lis_txt = ['%d'%(cell_val), per_ok_s, '%.2f%%'%(per_err)]\n        lis_kwa = [text_kwargs]\n        dic = text_kwargs.copy(); dic['color'] = 'b'; lis_kwa.append(dic);\n        dic = text_kwargs.copy(); dic['color'] = 'r'; lis_kwa.append(dic);\n        lis_pos = [(oText._x, oText._y-0.3), (oText._x, oText._y), (oText._x, oText._y+0.3)]\n        for i in range(len(lis_txt)):\n            newText = dict(x=lis_pos[i][0], y=lis_pos[i][1], text=lis_txt[i], kw=lis_kwa[i])\n            #print 'lin: %s, col: %s, newText: %s' %(lin, col, newText)\n            text_add.append(newText)\n        #print '\\n'\n\n        #set background color for sum cells (last line and last column)\n        carr = [0.27, 0.4, 0.4, 1.0]\n        if(col == ccl - 1) and (lin == ccl - 1):\n            carr = [0.2, 0.20, 0.17, 1.0]\n        facecolors[posi] = carr\n\n    else:\n        if(per > 0):\n            txt = '%s\\n%.2f%%' %(cell_val, per)\n        else:\n            if(show_null_values == 0):\n                txt = ''\n            elif(show_null_values == 1):\n                txt = '0'\n            else:\n                txt = '0\\n0.0%'\n        oText.set_text(txt)\n\n        #main diagonal\n        if(col == lin):\n            #set color of the textin the diagonal to white\n            oText.set_color('w')\n            # set background color in the diagonal to blue\n            facecolors[posi] = [0.35, 0.42, 0.7, 1]\n        else:\n            oText.set_color('r')\n\n    return text_add, text_del\n#\n\ndef insert_totals(df_cm):\n    \"\"\" insert total column and line (the last ones) \"\"\"\n    sum_col = []\n    for c in df_cm.columns:\n        sum_col.append( df_cm[c].sum() )\n    sum_lin = []\n    for item_line in df_cm.iterrows():\n        sum_lin.append( item_line[1].sum() )\n    df_cm['sum_lin'] = sum_lin\n    sum_col.append(np.sum(sum_lin))\n    df_cm.loc['sum_col'] = sum_col\n    #print ('\\ndf_cm:\\n', df_cm, '\\n\\b\\n')\n#\n\ndef pretty_plot_confusion_matrix(df_cm, annot=True, cmap=\"Blues\", fmt='.2f', fz=15,\n      lw=0.5, cbar=False, figsize=[10,10], show_null_values=0, pred_val_axis='y'):\n    \"\"\"\n      print conf matrix with default layout (like matlab)\n      params:\n        df_cm          dataframe (pandas) without totals\n        annot          print text in each cell\n        cmap           Oranges,Oranges_r,YlGnBu,Blues,RdBu, ... see:\n        fz             fontsize\n        lw             linewidth\n        pred_val_axis  where to show the prediction values (x or y axis)\n                        'col' or 'x': show predicted values in columns (x axis) instead lines\n                        'lin' or 'y': show predicted values in lines   (y axis)\n    \"\"\"\n    if(pred_val_axis in ('col', 'x')):\n        xlbl = 'Predicted'\n        ylbl = 'Actual'\n    else:\n        xlbl = 'Actual'\n        ylbl = 'Predicted'\n        df_cm = df_cm.T\n\n    # create \"Total\" column\n    insert_totals(df_cm)\n\n    #this is for print allways in the same window\n    fig, ax1 = get_new_fig('Conf matrix default', figsize)\n\n    #thanks for seaborn\n    ax = sn.heatmap(df_cm, annot=annot, annot_kws={\"size\": fz}, linewidths=lw, ax=ax1,\n                    cbar=cbar, cmap=cmap, linecolor='w', fmt=fmt)\n\n    #set ticklabels rotation\n    ax.set_xticklabels(ax.get_xticklabels(), rotation = 0, fontsize = 12)\n    ax.set_yticklabels(ax.get_yticklabels(), rotation = 0, fontsize = 12)\n\n    # Turn off all the ticks\n    for t in ax.xaxis.get_major_ticks():\n        t.tick1On = False\n        t.tick2On = False\n    for t in ax.yaxis.get_major_ticks():\n        t.tick1On = False\n        t.tick2On = False\n\n    #face colors list\n    quadmesh = ax.findobj(QuadMesh)[0]\n    facecolors = quadmesh.get_facecolors()\n\n    #iter in text elements\n    array_df = np.array( df_cm.to_records(index=False).tolist() )\n    text_add = []; text_del = [];\n    posi = -1 #from left to right, bottom to top.\n    for t in ax.collections[0].axes.texts: #ax.texts:\n        pos = np.array( t.get_position()) - [0.5,0.5]\n        lin = int(pos[1]); col = int(pos[0]);\n        posi += 1\n        #print ('>>> pos: %s, posi: %s, val: %s, txt: %s' %(pos, posi, array_df[lin][col], t.get_text()))\n\n        #set text\n        txt_res = configcell_text_and_colors(array_df, lin, col, t, facecolors, posi, fz, fmt, show_null_values)\n\n        text_add.extend(txt_res[0])\n        text_del.extend(txt_res[1])\n\n    #remove the old ones\n    for item in text_del:\n        item.remove()\n    #append the new ones\n    for item in text_add:\n        ax.text(item['x'], item['y'], item['text'], **item['kw'])\n\n    #titles and legends\n    ax.set_title('Confusion matrix')\n    ax.set_xlabel(xlbl)\n    ax.set_ylabel(ylbl)\n    plt.tight_layout()  #set layout slim\n    plt.show()\n#\n\ndef plot_confusion_matrix_from_data(y_test, predictions, columns=None, annot=True, cmap=\"Blues\",\n      fmt='.2f', fz=11, lw=0.5, cbar=False, figsize=[10,10], show_null_values=0, pred_val_axis='lin'):\n    \"\"\"\n        plot confusion matrix function with y_test (actual values) and predictions (predic),\n        whitout a confusion matrix yet\n    \"\"\"\n    from sklearn.metrics import confusion_matrix\n    from pandas import DataFrame\n\n    #data\n    if(not columns):\n        #labels axis integer:\n        ##columns = range(1, len(np.unique(y_test))+1)\n        #labels axis string:\n        from string import ascii_uppercase\n        columns = ['class %s' %(i) for i in list(ascii_uppercase)[0:len(np.unique(y_test))]]\n\n    confm = confusion_matrix(y_test, predictions)\n    #cmap = 'Blues';\n    #fz = 11;\n    #figsize=[9,9];\n    show_null_values = 2\n    df_cm = DataFrame(confm, index=columns, columns=columns)\n    pretty_plot_confusion_matrix(df_cm, fz=fz, cmap=cmap, figsize=figsize, show_null_values=show_null_values, pred_val_axis=pred_val_axis)\n#\n\n\n\n#\n#TEST functions\n#\ndef _test_cm():\n    #test function with confusion matrix done\n    array = np.array( [[13,  0,  1,  0,  2,  0],\n                       [ 0, 50,  2,  0, 10,  0],\n                       [ 0, 13, 16,  0,  0,  3],\n                       [ 0,  0,  0, 13,  1,  0],\n                       [ 0, 40,  0,  1, 15,  0],\n                       [ 0,  0,  0,  0,  0, 20]])\n    #get pandas dataframe\n    df_cm = DataFrame(array, index=range(1,7), columns=range(1,7))\n    #colormap: see this and choose your more dear\n    cmap = 'PuRd'\n    pretty_plot_confusion_matrix(df_cm, cmap=cmap)\n#\n\ndef _test_data_class():\n    \"\"\" test function with y_test (actual values) and predictions (predic) \"\"\"\n    #data\n    y_test = np.array([1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5])\n    predic = np.array([1,2,4,3,5, 1,2,4,3,5, 1,2,3,4,4, 1,4,3,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,3,3,5, 1,2,3,3,5, 1,2,3,4,4, 1,2,3,4,1, 1,2,3,4,1, 1,2,3,4,1, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,4,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5, 1,2,3,4,5])\n    \"\"\"\n      Examples to validate output (confusion matrix plot)\n        actual: 5 and prediction 1   >>  3\n        actual: 2 and prediction 4   >>  1\n        actual: 3 and prediction 4   >>  10\n    \"\"\"\n    columns = []\n    annot = True;\n    cmap = 'PuRd';\n    fmt = '.2f'\n    lw = 0.5\n    cbar = False\n    show_null_values = 2\n    pred_val_axis = 'y'\n    #size::\n    fz = 12;\n    figsize = [10,10];\n    if(len(y_test) > 10):\n        fz=9; figsize=[14,14];\n    plot_confusion_matrix_from_data(y_test, predic, columns,\n      annot, cmap, fmt, fz, lw, cbar, figsize, show_null_values, pred_val_axis)\n#\n\n\n#\n#MAIN function\n#\nif(__name__ == '__main__'):\n    print('__main__')\n    print('_test_cm: test function with confusion matrix done\\nand pause')\n    _test_cm()\n    plt.pause(5)\n    print('_test_data_class: test function with y_test (actual values) and predictions (predic)')\n    _test_data_class()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dc87443bebdd2f21bd5c32ae26cff3422290d88"},"cell_type":"markdown","source":"# 3. Split Train and Test"},{"metadata":{"trusted":true,"_uuid":"964b5be4a50c8f8cc4b82a764aa0c98e70c9ab3e"},"cell_type":"code","source":"df1 = full_dataset.copy(deep=True)\ndf1 = df1.dropna()\n#create y from stock returns for next 10 days variable.\ny = df1.returnsOpenNextMktres10 > 0\n# Rest of the dataset is X\n\n#cols = [ 'volume','returnsClosePrevMktres1-20DMA',\\\n#       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\\\n#       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\\\n#       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\\\n#       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\\\n#       'assetCodeT',\\\n#       'volumeCounts7D', 'rsi20D',\\\n#       'volume10DMA', 'close10DMA', 'sentenceCount7D',\\\n#       'firstMentionSentence7D', 'relevance7D', 'sentimentWordCount7D',\\\n#       'sentimentClass7D', 'urgency7D', 'takeSequence7D', 'companyCount7D',\\\n#       'close_10EMA', 'close_26EMA', 'close_12EMA',\\\n#       'MACD', 'MA_10MA', 'MA_10MA_std', 'MA_10MA_BB_high', 'MA_10MA_BB_low',\\\n#       'Year', 'Month', 'Day', 'Week', 'day_of_year' ]\n\n\n#df1[cols].head(200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15913ae2c738d3ad95f2f077b4c7d6f0dcffbdc5"},"cell_type":"code","source":"cols = [\n       'volume10DMA'\n    ,'day_of_year'\\\n    ,'MA_10MA_BB_high'\\\n    ,'close_10EMA'\\\n    ,'returnsClosePrevMktres10'\\\n    ,'MACD'\\\n    ,'companyCount7D'\\\n    ,'sentimentClass7D'\\\n    ,'rsi10D'\\\n    #,'assetCodeT'\\\n    #,'close_to_open'\\\n    ,'volumeCounts7D'\\\n    #'returnsClosePrevMktres1-20DMA'\\\n    ,'returnsClosePrevMktres1-yearly'\\\n    ,'returnsClosePrevMktres1-quarterly'\\\n    #'returnsOpenPrevMktres1-20DMA'\\\n    ,'returnsOpenPrevMktres1-yearly'\\\n    ,'returnsOpenPrevMktres1-quarterly'\\\n   \n    \n]\nlen(cols)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"497e081c23b10f75312cbde22fdebac3c7889ae9","trusted":true},"cell_type":"code","source":"\n\nX = df1[cols] \n#train_size = int(len(X) * 0.66)\n#X_train, X_test = X[0:train_size], X[train_size:len(X)]\n#y_train, y_test = y[0:train_size], y[train_size:len(X)]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=1)\n\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.34, random_state=1)\n\nprint('Observations: %d' % (len(X)))\nprint('Training Observations: %d' % (len(X_train)))\n#print('Validation Observations: %d' % (len(X_val)))\nprint('Testing Observations: %d' % (len(X_test)))\n\n# The target is binary\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c490bbfeb301f91432c3c3b02c3aa7b53a352a2"},"cell_type":"code","source":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    xgb.XGBClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #GLM\n    linear_model.LogisticRegressionCV(),\n\n    #Navies Bayes\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n   \n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f483f7ed29f9c9649f23575a3efa7b2b3abb76ca"},"cell_type":"code","source":"MLA_columns = []\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n\nrow_index = 0\nfor alg in MLA:\n    \n    \n    predicted = alg.fit(X_train, y_train).predict(X_test)\n    fp, tp, th = metrics.roc_curve(y_test, predicted)\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index,'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(X_train, y_train), 4)\n    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(X_test, y_test), 4)\n    #MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(y_test, predicted)\n    #MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(y_test, predicted)\n    MLA_compare.loc[row_index, 'MLA AUC'] = metrics.auc(fp, tp)\n\n\n\n\n\n    row_index+=1\n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    \nMLA_compare","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a36377d66ec1aed77a2d9cb69cff8b00d3b44fb6"},"cell_type":"code","source":"plt.subplots(figsize=(15,6))\nsns.barplot(x=\"MLA Name\", y=\"MLA AUC\",data=MLA_compare,palette=\"Blues\",edgecolor=sns.color_palette('Blues',7))\nplt.xticks(rotation=90)\nplt.title('Machine Learning Algorithms Test AUC Comparison')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c3b37fc6ebe46e95e85b368188540c0e6f39cd"},"cell_type":"markdown","source":"# Training AUC of full training Dataset"},{"metadata":{"_uuid":"821544ca45840e108553120096f7c9ce4947dc5b"},"cell_type":"markdown","source":"## XGBoost - Classifier 1"},{"metadata":{"trusted":true,"_uuid":"8d634722bb7ddcddd04691d3793ae7d9594f39d0"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport numpy as np\n \nxgb_model_train = xgb.XGBClassifier()\n \nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nfrom sklearn.metrics  import accuracy_score, roc_auc_score\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4b43af80ca16dfd4d7e1a9c9f46e7ad03ee85740"},"cell_type":"markdown","source":"plot_importance(model) \nplt.rcParams['figure.figsize'] = [30, 30]\nplt.rcParams['font.size'] = 40\nplt.show()"},{"metadata":{"_uuid":"9591eea10cf1b491295982cbbaa1ee26f811f9cb"},"cell_type":"markdown","source":"## Random Forest - Classifier 2"},{"metadata":{"trusted":true,"_uuid":"14685dc187da3f9f63e5a433764e2c4f20b02497"},"cell_type":"code","source":"rf_obj=RandomForestClassifier()\n#rf_obj.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a65b34e40fe166734ff9f1fe3b2add61bc9f5e35"},"cell_type":"markdown","source":"## BaggingClassifier - Classifier 3"},{"metadata":{"trusted":true,"_uuid":"bc2d75d2253b4078822e49f9f01505db0333acfc"},"cell_type":"code","source":"bgc_obj=BaggingClassifier(base_estimator=None)\n#bgc_obj.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7952be7b3dfae3023f3c590057ee490799ce1690"},"cell_type":"markdown","source":"## AUC Scores"},{"metadata":{"trusted":true,"_uuid":"275b810250324430ad6cdb1613f13464e887d223"},"cell_type":"markdown","source":"# Compare Algorithms\nimport pandas\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# prepare configuration for cross validation test harness\nseed = 7\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('BAGC', BaggingClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('ABC', AdaBoostClassifier()))\nmodels.append(('XGBC', xgb.XGBClassifier()))\nmodels.append(('GBC', GradientBoostingClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'roc_auc'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n"},{"metadata":{"trusted":true,"_uuid":"0a27d6f249bc976907e24ddefeff7bfb546fa5c3"},"cell_type":"markdown","source":"# boxplot algorithm comparison\nimport matplotlib as mpl\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(15, 15), dpi=80, facecolor='b', edgecolor='k')\n#figure(figsize=(1,1))\nfig = plt.figure()\nmpl.style.use('bmh')\nfig.set_size_inches(15, 15, forward=True)\nfig.suptitle('ML Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()"},{"metadata":{"_uuid":"64f8145eaae6cd9ab1941dd63b70e2f7d7a6b328"},"cell_type":"markdown","source":"# 5. Fit Classifier with GridSearchCV"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"3763a9f579091b17261b9fe06eff9df9fc4d8e77"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nimport numpy as np ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3318f15aec5cee4f26d6b1ecfdd8a0110653d00c"},"cell_type":"code","source":"#brute force scan for all parameters, here are the tricks\n#usually max_depth is 6,7,8\n#learning rate is around 0.05, but small changes may make big diff\n#tuning min_child_weight subsample colsample_bytree can have \n#much fun of fighting against overfit \n#n_estimators is how many round of boosting\n#finally, ensemble xgboost with multiple seeds may reduce variance\n#from sklearn.model_selection import TimeSeriesSplit\n#tss = TimeSeriesSplit(n_splits=10).split(X_train)\n\nparams = {\n         \n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'max_depth': [6,7,8,9,10],\n        'n_estimators': [50,100  ]  \n        }\n \n#gsearch = GridSearchCV(xgb_model_train, params, n_jobs=5,cv=5, scoring='roc_auc',verbose=1, refit=True)\ngsearch = RandomizedSearchCV(xgb_model_train, params, cv = 5, scoring = 'roc_auc', verbose=1, \n                              random_state=42, n_jobs = -1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf4bc8cc2712ab182afed857d16df54acc1b3a78"},"cell_type":"markdown","source":"## Random Forest - Classifier 2 - Cross Val"},{"metadata":{"trusted":true,"_uuid":"03c047054284e67ab46d71a69753b263fa22739a"},"cell_type":"code","source":"params = { \n'max_depth': [14,15,16,17],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [50,100,200,300,400] \n}\n#rf_Grid = GridSearchCV(rf_obj, param_grid, cv = 5, scoring = 'roc_auc',refit = True, n_jobs=-1, verbose = 1)\nrf_Grid = RandomizedSearchCV(rf_obj, params,cv = 5, scoring = 'roc_auc', verbose=1, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09164e16f1f33e61090c01ef5f02de82954327f9"},"cell_type":"markdown","source":"## BaggingClassifier - Classifier 3 - Cross Val"},{"metadata":{"trusted":true,"_uuid":"880283b7b620aacfbdc5b95ce40489fafd93adb3"},"cell_type":"markdown","source":"params = {\n  'max_depth': [5,6, 7,8],\n  'max_samples' : [0.05, 0.1, 0.2, 0.5],\n  'n_estimators': [50,100, 200, 400] ,\n  'bootstrap' :[True,False],\n  'bootstrap_features':[True,False]\n}\n \nbgc_Grid = RandomizedSearchCV(BaggingClassifier(DecisionTreeClassifier(), max_features = 0.5), params , cv = 5, scoring = 'roc_auc', verbose=1, random_state=42, n_jobs = -1)"},{"metadata":{"_uuid":"d8320653dbbbe918c2ca2fe2774b48b7042c5855"},"cell_type":"markdown","source":"## AdaBoost - Classifier 4 - Cross Val"},{"metadata":{"trusted":true,"_uuid":"7547815a2fb4f6647a11efa3959efd4a28a80cff"},"cell_type":"markdown","source":"param_grid = {\n'n_estimators' :[ 100, 200, 300, 400],\n'learning_rate': [0.2,0.4,0.6,0.8,1, 1.2]\n}\nab_Grid = GridSearchCV(ab_obj, param_grid, cv = 10, scoring = 'roc_auc',refit = True, n_jobs=-1, verbose = 1)"},{"metadata":{"_uuid":"0284161b3a820506e5a47d2e67197ad66040ace5"},"cell_type":"markdown","source":"# # **7. Use GridSearchCV to tune hyper parameters.**"},{"metadata":{"trusted":true,"_uuid":"5a141dbf21013ad1b9b5df34a7ed469237852948","scrolled":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\ngsearch.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d864449ca83445f9f4e6dbe3040e9d5a0ca9ce30","scrolled":false},"cell_type":"code","source":"rf_Grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e84e03db320d2bf1b803ba305c61c56b2bbe779"},"cell_type":"markdown","source":"bgc_Grid.fit(X_train, y_train)"},{"metadata":{"trusted":true,"_uuid":"95cc0cc0b7d8c9e2e52625e1b2528797727cfb6d"},"cell_type":"code","source":"\n\nplot_importance(gsearch.best_estimator_) \nplt.rcParams['figure.figsize'] = [12,12]\nplt.rcParams['font.size'] = 25\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a8bdd231e67770140c734e9186c479b988736f"},"cell_type":"code","source":"\ngsearch.best_estimator_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7888ede288165328b1e9ba651336a4d72ecb7e32"},"cell_type":"code","source":"xg_clf = gsearch.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f92f3da8b33a1c611e1a67fc386558ad9d97f2d"},"cell_type":"code","source":"rf_clf = rf_Grid.best_estimator_\nrf_clf ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05d48bb8b4051d0f407f4cde1fad40759f96e44e"},"cell_type":"code","source":"feature_importances = pd.Series(rf_clf .feature_importances_, index=X_train.columns)\nfeature_importances.nlargest(10).sort_values(ascending = True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2f04d122b85d259b7adba6117dd332587ea16a1"},"cell_type":"markdown","source":"bgc_clf = bgc_Grid.best_estimator_"},{"metadata":{"trusted":true,"_uuid":"9c20c4db9d7ba02b28e134ad3b4eb28e1f107dd7"},"cell_type":"code","source":"#ab_clf=ab_Grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d3eb27032210e4c5f8308bb38cc0d328c8cd083"},"cell_type":"markdown","source":"# 8. Validation set AUC"},{"metadata":{"trusted":true,"_uuid":"66d401057d5c8ddefadf7d75e0fb941cd9a56ae9"},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n#trust your CV!\nbest_parameters, score, _ = max(gsearch.grid_scores_, key=lambda x: x[1])\nprint('XGBoost Cross validation AUC score:', np.round(gsearch.best_score_ ,2))\nfor param_name in sorted(best_parameters.keys()):\n   print(\"%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e315014eb858e88526439a08b715dad352a465ea"},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n#trust your CV!\n#best_parameters, score, _ = max(rf_Grid.grid_scores_, key=lambda x: x[1])\nprint('RandomForest Cross validation AUC score:', np.round(rf_Grid.best_score_ ,2))\n#for param_name in sorted(best_parameters.keys()):\n#    print(\"%s: %r\" % (param_name, best_parameters[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9f1e37dda38e2fa84572c668da4a6deb25b5af9","_kg_hide-output":false},"cell_type":"markdown","source":"#from sklearn.metrics import accuracy_score\n#trust your CV!\n#best_parameters, score, _ = max(bgc_Grid.grid_scores_, key=lambda x: x[1])\nprint('Bagging Classifier Cross validation AUC score:', np.round(bgc_Grid.best_score_ ,2))\n#for param_name in sorted(best_parameters.keys()):\n#    print(\"%s: %r\" % (param_name, best_parameters[param_name]))"},{"metadata":{"_uuid":"22163bb4774c60a29574168df1a523d74244c39c"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"40a703baf57b052907d800256eb6faf52868681e"},"cell_type":"markdown","source":"print('XgBoost Training AUC score:', np.round(roc_auc_score(clf.predict(X_train),y_train),2))\nplot_importance(clf, max_num_features=20) # top 20 most important features\nplt.show()\n\nprint('RandomForest Training AUC score:', np.round(roc_auc_score(rf_clf.predict(X_train),y_train),2))\n\n\n"},{"metadata":{"trusted":true,"_uuid":"f915086e5fdd58ecbf24d6f3971aba935fe41c21"},"cell_type":"markdown","source":"print('BaggingClassifier Training AUC score:', np.round(roc_auc_score(rf_clf.predict(X_train),y_train),2))\n\n\nprint('Adaboost Training AUC score:', np.round(roc_auc_score(ab_clf.predict(X_train),y_train),2))\n"},{"metadata":{"_uuid":"2a6b1b07cf14e981b250c0c795b919b3b84b6001"},"cell_type":"markdown","source":"# Cross Validate"},{"metadata":{"trusted":true,"_uuid":"b4aa247e61c0cc73099863299938706bcae4afed"},"cell_type":"markdown","source":"seed = 7\nscoring = 'roc_auc'\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\ncv_results = model_selection.cross_val_score(rf_clf, X_val, y_val, cv=kfold, scoring=scoring) \n"},{"metadata":{"trusted":true,"_uuid":"2b95884667c54af089efab02d1223b937199a7f6"},"cell_type":"markdown","source":"msg = \" %f (%f)\" % ( cv_results.mean(), cv_results.std())\nprint(msg)"},{"metadata":{"trusted":true,"_uuid":"cd1a0fbd3da883fd28b0be46668c48483212ddc3"},"cell_type":"markdown","source":"\n\nscoring = 'roc_auc'\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\ncv_results = model_selection.cross_val_score(clf, X_val, y_val, cv=kfold, scoring=scoring) \nmsg = \"%f (%f)\" % (cv_results.mean(), cv_results.std())\nprint(msg)\n\n\n"},{"metadata":{"trusted":true,"_uuid":"e20e3ef7c3371d066bea64ac97e3a65a849f948e"},"cell_type":"markdown","source":"scoring = 'roc_auc'\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\ncv_results = model_selection.cross_val_score(bgc_clf, X_val, y_val, cv=kfold, scoring=scoring) \nmsg = \" %f (%f)\" % ( cv_results.mean(), cv_results.std())\nprint(msg)"},{"metadata":{"trusted":true,"_uuid":"fa74d1f57b9c36a93fe0752896d7c6bdcfc920ab","_kg_hide-output":false},"cell_type":"code","source":"max_depths = [  3,  4,  5,  6, 7,  8,  9, 10, 11, 12,13,14,15,16,18]\n\nprint (max_depths)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cacd0c7a4d71fa9618764f9e3ef96abd608fb00"},"cell_type":"code","source":"train_results = []\ntest_results = []\nfrom sklearn.model_selection import cross_val_score\nfor max_depth in max_depths:\n   dt = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=1, learning_rate=0.1, max_delta_step=0,\n       max_depth=max_depth, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1 )  \n   scores = cross_val_score(dt, X_train, y_train, cv=5,scoring=\"accuracy\")\n   dt.fit(X_train,y_train)\n   #print(\"estimated AUC on the XG Boost training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))   \n   # Add auc score to previous train results\n   train_results.append(scores.mean())\n   #y_pred = dt.predict(X_test)\n   #false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n   #roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n   testAccuracy = round(dt.score(X_test, y_test), 4)\n   # Add auc score to previous test results\n   test_results.append(testAccuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0f4d03d4d1a6d5707c2d06d0893cdfa87913207"},"cell_type":"code","source":"from matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Scores for XGBoost')\nplt.xlabel('Tree depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e187b293f7c6c66ce873414b7b53db95a6c8be80"},"cell_type":"code","source":"train_results = []\ntest_results = []\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfor max_depth in max_depths:\n   dt = RandomForestClassifier(max_depth=max_depth)\n   \n   scores = cross_val_score(dt, X_train, y_train, cv=5,scoring=\"accuracy\")\n   #print(\"estimated AUC on the RF training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n   train_results.append(scores.mean())\n   dt.fit(X_train,y_train)\n   #y_pred = dt.predict(X_test)\n   #false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n   #roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous test results\n   testAccuracy = round(dt.score(X_test, y_test), 4)\n   # Add auc score to previous test results\n   test_results.append(testAccuracy)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score for RFClassifier')\nplt.xlabel('Tree depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"faaa23e254c634b01e8e9dc76757031011691b33"},"cell_type":"code","source":"train_results = []\ntest_results = []\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfor max_depth in max_depths:\n   dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth = max_depth))\n   scores = cross_val_score(dt, X_train, y_train, cv=5,scoring=\"roc_auc\")\n   #print(\"estimated AUC on the RF training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n   train_results.append(scores.mean())\n   dt.fit(X_train,y_train)\n  #y_pred = dt.predict(X_test)\n   #false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred)\n   #roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n   # Add auc score to previous test results\n   testAccuracy = round(dt.score(X_test, y_test), 4)\n   # Add auc score to previous test results\n   test_results.append(testAccuracy)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('Score for Bagging Classifier')\nplt.xlabel('Tree depth')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"40765942c69d1e627c8f0fb8e31ba038893e7361"},"cell_type":"markdown","source":"# plot learning curve\nfrom numpy import loadtxt\n \n# load data\nmodel = xgb.XGBClassifier()\neval_set = [(X_train, y_train), (X_test, y_test)]\nmodel.fit(X_train, y_train, eval_metric=[\"error\", \"auc\"], eval_set=eval_set, verbose=False)\n# make predictions for test data\nprobs =  model.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"AUC: %.2f%%\" % (roc_auc * 100.0))\n# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n# plot log loss\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Test')\nax.legend()\npyplot.ylabel('auc')\npyplot.title('XGBoost auc')\npyplot.show()\n# plot classification error\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\npyplot.ylabel('Classification Error')\npyplot.title('XGBoost Classification Error')\npyplot.show()"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"da54151b2136b5c6a12fdf326aa631c2591c0e67"},"cell_type":"markdown","source":"eval_set = [(X_test, y_test)]\nmodel.fit(X_train, y_train, early_stopping_rounds=30, max_depth=9,eval_metric=\"auc\", eval_set=eval_set, verbose=False)"},{"metadata":{"_uuid":"383b6807b4bdde332b811b416f79f7d82c2a67f1"},"cell_type":"markdown","source":"# Train Predictions"},{"metadata":{"trusted":true,"_uuid":"e703a6c8e3b2a42e94bd790441d9bd132f88a258"},"cell_type":"code","source":" \nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(rf_clf, X_train, y_train, cv=5,scoring=\"roc_auc\")\nprint(\"estimated AUC on the training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06b2b0d8a0dfece8f016698742d449eed5acf0c0"},"cell_type":"markdown","source":"probsTrain =  bgc_clf.predict_proba(X_train)\npredsTrain = probsTrain[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_train, predsTrain)\nroc_auc = metrics.auc(fpr, tpr)\nprint ('Train AUC Score for Bagging Classifier', roc_auc)\n\n"},{"metadata":{"trusted":true,"_uuid":"50f80ee9a2ce2c0621332dac9df21bcd522f8922"},"cell_type":"code","source":" \nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(xg_clf, X_train, y_train, cv=5,scoring=\"roc_auc\")\nprint(\"estimated AUC on the training data-set: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca1f8b0a2f5cd9488513297330e244c9308035b2"},"cell_type":"markdown","source":"# Test set AUC for XGBoost"},{"metadata":{"trusted":true,"_uuid":"7fdf7c55e88a2fcf4779feddffa9664bc1738ca1"},"cell_type":"code","source":"# calculate the fpr and tpr for all thresholds of the classification\nprobs =  gsearch.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nprint (\"Test AUC for XGBoost\", roc_auc )\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic XGBoost Test')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34e9428abab696c28576ce910df1d52c0d2a1ac4"},"cell_type":"code","source":"y_scores = gsearch.predict(X_test)\n\nplot_confusion_matrix_from_data(y_test,  y_scores,columns=[\"Up\",\"Down\" ], annot=True, cmap=\"Blues\",\n      fmt='.5f', fz=20, lw=1, cbar=False, figsize=[12,12], show_null_values=0, pred_val_axis='lin')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d802815adaeb98243c7e550b48bfdf7fdd56f45"},"cell_type":"code","source":"clfReport = metrics.classification_report(y_test, y_scores, target_names=[\"Stock-Movement-Up\", \"Stock-Movement-Down\"])\nprint(clfReport)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de5ea3f95bcc46e38d6bb8ede85dc8d1dd3affc8"},"cell_type":"code","source":"# calculate the fpr and tpr for all thresholds of the classification\nprobs =  rf_clf.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nprint (\"Test AUC for Random Forest\", roc_auc )\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic for RF Test')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\ny_scores = rf_clf.predict(X_test)\n\nplot_confusion_matrix_from_data(y_test,  y_scores,columns=[\"Up\",\"Down\" ], annot=True, cmap=\"Blues\",\n      fmt='.5f', fz=20, lw=1, cbar=False, figsize=[12,12], show_null_values=0, pred_val_axis='lin')\n\nclfReport = metrics.classification_report(y_test, y_scores, target_names=[\"Stock-Movement-Up\", \"Stock-Movement-Down\"])\nprint(clfReport)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1b4b69e0d1921983dd73b367cdb0e937318e3fc"},"cell_type":"code","source":"def get_models():\n    \"\"\"Generate a library of base learners.\"\"\" \n    models = {'XG Boost': xg_clf,\n              'Random Forest': rf_clf \n              }\n\n    return models\n\n\ndef train_predict(model_list):\n    \"\"\"Fit models in list on training set and return preds\"\"\"\n    P = np.zeros((y_test.shape[0], len(model_list)))\n    P = pd.DataFrame(P)\n\n    print(\"Fitting models.\")\n    cols = list()\n    for i, (name, m) in enumerate(models.items()):\n        print(\"%s...\" % name, end=\" \", flush=False)\n        m.fit(X_train, y_train)\n        P.iloc[:, i] = m.predict_proba(X_test)[:, 1]\n        cols.append(name)\n        print(\"done\")\n\n    P.columns = cols\n    print(\"Done.\\n\")\n    return P\n\n\ndef score_models(P, y):\n    \"\"\"Score model in prediction DF\"\"\"\n    print(\"Scoring models.\")\n    for m in P.columns:\n        score = roc_auc_score(y, P.loc[:, m])\n        print(\"%-26s: %.3f\" % (m, score))\n    print(\"Done.\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8faa30da6e6d805494b8496adf68f259f452427f"},"cell_type":"code","source":"models = get_models()\nP = train_predict(models)\nscore_models(P, y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35170b54cbfab46c52ab6c5cc02cedca6f799199"},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\ndef plot_roc_curve(ytest, P_base_learners,   labels, ens_label):\n    \"\"\"Plot the roc curve for both base learners .\"\"\"\n    plt.figure(figsize=(10, 8))\n    plt.plot([0, 1], [0, 1], 'k--')\n    \n    cm = [plt.cm.rainbow(i)\n      for i in np.linspace(0, 1.0, P_base_learners.shape[1] + 1)]\n    \n    for i in range(P_base_learners.shape[1]):\n        p = P_base_learners[:, i]\n        fpr, tpr, _ = roc_curve(y_test, p)\n        roc_auc = metrics.auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=P.columns[i] + ' AUC = %0.2f' % roc_auc, c=cm[i + 1])\n         \n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend()    \n    plt.show()\n\n\nplot_roc_curve(y_test, P.values, P.mean(axis=1), \"ensemble\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b73bc56987de36cd310db7cd29b004493029748"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09a81cc03a35332ccc54491c609a1137b0787e4a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bfa96037599672b792f26c4e11e41f8612f331c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d101ee28b9421bd4c7be130e19ca2f85bbd27545"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}